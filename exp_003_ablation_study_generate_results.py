from genericpath import exists
import matplotlib.pyplot as plt
import numpy as np
import scipy.io as sio

import pickle

import utils.gcd_utils as gcd_utils 
import models.gcd_models as gcd_models
import models.gcd_vi_models as gcd_vi_models


""" Implement experiment in Sec. VI-D Ablation study on the number of superpixels and structure
learning algorithm, and store results for posterior visualization. """

import os 
dirname = os.path.dirname(__file__)
datasets_dir = os.path.join(dirname, 'datasets')

def parse_alg_params(n,epsilon,dict_name,norm,scheme):
    alg_param = {}
    if scheme == 'unsupervised':
        alg_param['C_params'] = (epsilon,norm)
        if dict_name in ['2-GCN','2-SGCN']:
            alg_param['basis_params'] = (dict_name, n)# int(n/16))  
            alg_param['feature_matrix'] = X_mean[0] 
        elif dict_name == 'RLP':
            alg_param['basis_params'] = (dict_name, n)  
        else:
            raise ValueError('alg_param must be set using a valid configuration.')
    elif scheme == 'semisupervised':
        alg_param['C_params'] = (norm,np.zeros((n,1)),epsilon)    
        alg_param['basis_params'] = (dict_name, n) 
        #alg_ofile_name = dict_name +'_' + norm            
        #alg = alg_name(alg_i,norm_j) 

    return alg_param

####


out_fn = lambda db_idx,db_name, ext: 'Exp_003_ablation_study_on_db_'+ str(db_idx) + '_' + db_name + ext
results_dir = os.path.join(dirname, 'results')
results_dir = os.path.join(results_dir,'ablation_study')


dataset_names = ['Alaska','California','Atlantico', 'Mulgaria','Toulouse','Shuguang']
gl_models = ['Kalofolias', 'MST_with_binary_weights']
#gl_models = ['MST_with_binary_weights']
#gl_models = ['Kalofolias']
num_superpixels = np.logspace(2,4,5).astype(int)

filename_format = '{i}_{scheme}_results_on_{db}_with_{glmodel}_{dict_name}_{norm}'
for dataset_idx in [0]:#range(1,len(dataset_names)):

    # Load change detection dataset
    dataset = gcd_utils.dataset_loader(datasets_dir,dataset_names[dataset_idx])

    

    # Create dataset from image pairs based on slic superpixels
    for i in range(len(num_superpixels)):
        
        # Apply preprocessing pipeline
        n_spixels = num_superpixels[i]
        context_radius = 3
        if dataset['metadata']['name'] in ['California','Mulgaria','Alaska','Atlantico','Toulouse-1']:
            prepro_params = {'sp_method':'slic','outlier_removal':True}
        else: #'Shuguang'
            prepro_params = {'sp_method':'slic','outlier_removal':False}
        X_mean, X, F, segments = gcd_utils.prepro_pipeline(dataset,n_spixels,context_radius,prepro_params)

        # Construct graph on which labels are assumed to be smooth    
        n = X[0].shape[0]
        d = X[0].shape[1]+X[1].shape[1]
        k = int(np.sqrt(n))

        for gl_model_idx in range(0,len(gl_models)):
            #scheme = 'unsupervised' 
            log_fn = out_fn(dataset_idx,dataset['metadata']['name'],'.txt')
            # Define gl_params for main graph inferred from pre and post event avg. signals. The other graph
            # is assumed to be a knn graph with Gaussian weights.
            gl_params = {'model': gl_models[gl_model_idx],
                        'k': k,
                        'knn_edge_cons': True,
                        'k_': 5*k,
                        'fusion_rule': 'sum',
                        'tol': 1e-5
                        }
            W, W1, outparams, outparams1 = gcd_utils.structure_learning_pipeline(X_mean,'unsupervised',gl_params,
                                                                                    log_fn,results_dir)
            
            epsilon = n*1e5 # just a very large number initially
            dict_name = 'RLP'
            norm = '2'

            for scheme in ['semisupervised','unsupervised']:
                alg_param = parse_alg_params(n,epsilon,dict_name,norm,scheme)
                alg_param['maxit'] = n
                alg_param['reversed'] = False
                alg_param['gamma0'] = 1

                # Convert graph signals to image
                if scheme == 'semisupervised':
                    sampling_params = {'type': 'stratified_random_sampling', 
                                       'seed' : 1094, 
                                       'labels': F[:,0]}
                    m = np.max([int(0.065*np.sum(sampling_params['labels'])),1])
                    y_train, idx_train = gcd_utils.generate_train_dataset(m,n,sampling_params)
                    f_hat, probs_hat = gcd_models.gssm_estimation(y_train,idx_train,W,alg_param)
                    score_map = gcd_utils.spixels_upsampling(probs_hat, segments) 
                    output = {'probs_hat':probs_hat}
                else:                    
                    x1_hat, delta, history = gcd_vi_models.unsupervised_gssm_estimation(X_mean[1],W1,W,alg_param)
                    delta_mag = np.sqrt(np.sum((delta)**2,axis=1)).reshape((n,1))    
                    score_map = gcd_utils.spixels_upsampling(delta_mag, segments) 
                    output = {'x1_hat':x1_hat,
                              'delta':delta,
                              'history':history}
    
                ref_fpr = 0.05
                metrics_dict = gcd_utils.compute_performance_metrics(dataset['gt'], score_map, ref_fpr)
                #trial = '{sche}_results_on_{db}_with_{glmodel}_{dict_name}_{norm}'
                result_fn = filename_format.format(i=i,scheme=scheme,
                                      db=dataset['metadata']['name'],
                                      glmodel=gl_models[gl_model_idx],
                                      dict_name=dict_name,
                                      norm=norm)+'.pkl'
                output['metrics_dict'] = metrics_dict
                output['segments'] = segments
                output['num_spixels_ref'] = n_spixels
                output['num_slic_spixels'] = n

                with open(os.path.join(results_dir, result_fn), 'wb') as r2f:
                    pickle.dump(output, r2f)




            
