import numpy as np
import pandas as pd
import pickle
import os 

import utils.gcd_utils as gcd_utils 
import utils.exp001_utils as exp001_utils


""" Implement experiment in Sec. VI-B Change probability and change map estimation from a few
labeled nodes, and store results for posterior visualization. """

dirname = os.path.dirname(__file__)
datasets_dir = os.path.join(dirname, 'datasets')
out_fn = lambda db_idx,db_name, ext: 'Exp_001_on_db_'+ str(db_idx) + '_' + db_name + ext

results_dir = os.path.join(dirname, 'results')
results_dir = os.path.join(results_dir,'FinalResults_13_02_2023')
#results_dir = os.path.join(results_dir,'test')


dataset_names = ['Alaska','California','Atlantico', 'Mulargia','Toulouse','Shuguang']
n_trials = 1
label_rate_cases = 3
for dataset_idx in [0,1,5,2,3,4]: #range(4,len(list_dataset_names)):

    dataset_name = dataset_names[dataset_idx]
    if os.path.isfile(os.path.join(results_dir,out_fn(dataset_idx,dataset_name,'.txt'))):
        continue
    # Load dataset
    dataset = gcd_utils.dataset_loader(datasets_dir,dataset_names[dataset_idx])

    # Create dataset from image pairs based on slic superpixels
    n_spixels = 2400
    context_radius = 2
    sp_method = 'slic'
    outlier_removal = True
    if dataset['metadata']['name'] in ['California','Mulargia','Alaska','Atlantico','Toulouse-1']:
        prepro_params = {'sp_method':'slic','iqr_clipping':True}
    else: #'Shuguang'
        prepro_params = {'sp_method':'slic','iqr_clipping':False}

    X_mean, X, F, segments = gcd_utils.prepro_pipeline(dataset,n_spixels,context_radius,prepro_params)

    # Define ground truth labeling function
    f_ask = dataset['gt']
    # Define training labels 
    f = 1 - np.argmax(F,axis=1)
    n_pos = np.sum(f)
    n_neg = np.sum(1-f)


    ## Construct graph on which labels are assumed to be smooth
    

        #X = X/scale
        #X_mean = np.hstack(X_mean)
        #avg_scale = np.max(X_mean,axis=0).reshape((1,X_mean.shape[1]))
        #X_mean = X_mean/avg_scale

        #scale = np.
        #X[0] = X[0]/np.max(X[0],axis=1)
        #X[1] = X[1]/np.max(X[1],axis=1)
        #X = np.hstack((X[0]/np.max(X[0],axis=1).reshape((n,1)),X[1]/np.max(X[1],axis=1).reshape((n,1))))

    #k_list = [int(n_spixels/32), int(n_spixels/4)]
    #k_list = [int(n/32), int(n/4)]
    #n = X[0].shape[0]
    #d = X[0].shape[1]+X[1].shape[1]
    n = X[0].shape[0]
    d = X[0].shape[1] + X[1].shape[1]
    k_list = [int(np.sqrt(n))]
    k_idx = 0

#result_file_path = results_dir + out_fn(dataset_idx,dataset_name,'.txt')
    mode = 'supervised'
    log_fn = out_fn(dataset_idx,dataset['metadata']['name'],'.txt')
    # Define gl_params for main graph inferred from pre and post event avg. signals. The other graph
    # is assumed to be a knn graph with Gaussian weights.
    gl_params = {'model': 'Kalofolias', # or 'Gaussian'
                'k': int(k_list[k_idx]),
                 'knn_edge_cons': True,
                 'k_': 5*k_list[k_idx],
                 'fusion_rule': 'sum',
                'tol': 1e-5
                 }
    W, outparams = gcd_utils.structure_learning_pipeline(X_mean,mode,gl_params,
                                                         log_fn,results_dir)
    
    result_file_path = os.path.join(results_dir,log_fn)
    
    if isinstance(X,tuple):
        X = np.hstack((X[0]/outparams['sigma1'],X[1]/outparams['sigma2']))

    rfile = open(result_file_path, 'w')
    rfile.write(dataset_name +'\n')  
    rfile.write('\nGraph learning params: \n')   
    rfile.write('-Method: ' + gl_params['model'] + '\n' ) 
    rfile.write('-Number of nodes (superpixels): ' + str(n) + '\n' )
    rfile.write('-Number of nearest neighbors: ' + str(gl_params['k']) + '\n' )

    rfile.write('\nFeature matrix characteristics: \n') 
    rfile.write('-Context radius (r): ' + str(context_radius) + '\n' ) 
    rfile.write('-Feature dimension (d): ' + str(d) + '\n' )  

    rfile.write('\nLabel function info:\n')
    rfile.write('-Number of positive samples (superpixels):'+str(n_pos)+'\n')
    rfile.write('-Number of negative samples (superpixels):'+str(n_neg)+'\n')
    rfile.close()

    # Define list of standard algorithms and proposed algorithm configurations 
    #alg_list = ['LP','2-GCN','2-SGCN','KMeans(RLP)','KMeans(2-GCN)']      
    #alg_list = alg_list + exp001_utils.create_and_return_proposed_alg_list(['RLP','GCN-2'],['1','2','inf']) 
    alg_list = ['LP','2-GCN','2-SGCN']    
    #alg_list = ['LP']  
    alg_list = alg_list + exp001_utils.create_and_return_proposed_alg_list(['RLP','2-GCN'],['2','inf']) 

    # Define number of trials and training set sizes
    n_trials = 10

    # Create a loop to evaluate different label fractions
    fraction_pos_samples = [1/16] # list with elements in (0,1) denoting the fraction of positive samples to be used.
    m = []
    for ps_idx in range(len(fraction_pos_samples)):        

        df, best_prob_map = exp001_utils.run_exp_001(X, W, f, f_ask, segments,
                                                        exp_params = {'label_fraction':fraction_pos_samples[ps_idx],
                                                                        'ntrials':n_trials,
                                                                        'alg_list':alg_list,
                                                                        'result_file_path':result_file_path})
        if ps_idx > 0:
            df_augmented = pd.concat([df_augmented,df],axis=1)
        else:
            df_augmented = df

        # Record best_prob_maps for a given m
        m.append(int(np.ceil(fraction_pos_samples[ps_idx]*n_pos)))
        with open(os.path.join(results_dir,out_fn(dataset_idx,dataset_name,'_best_prob_maps_'+'m_'+str(m[ps_idx])+'.pkl')), 'wb') as r2f:
                pickle.dump(best_prob_map, r2f)


    rfile = open(os.path.join(results_dir,out_fn(dataset_idx,dataset_name,'.txt')), 'a') 
    rfile.write('\nExperiment results:\n-Number of trials: '+str(n_trials)+'\n'+'-Training samples per class (m): ')
    for i  in range(len(m)-1):
        rfile.write(str(m[i])+',')
    rfile.write(str(m[len(m)-1])+'\n\n')
    rfile.write(df_augmented.to_latex())
    rfile.close()

# Display and save figs for m
#m = [14,4,17,4,18]
#m = [15,5,16,4,17,8]
#exp001_utils.display_semisup_results(datasets_dir,results_dir,dataset_names,m)
#exp001_utils.display_results(datasets_dir,results_dir,list_dataset_names,m)
        

        

