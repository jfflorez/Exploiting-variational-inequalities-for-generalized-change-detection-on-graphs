import numpy as np
import scipy as sp
import scipy.sparse as sp_sparse
import torch

import torch.nn.functional as functional
from torch_geometric.nn import GCNConv, SGConv
from sklearn import metrics

import matplotlib.pyplot as plt

import scipy.spatial.distance as sp_sd

import os.path
#from os import path
#import torch
from torch_cluster import knn_graph

# adding Folder_2 to the system path
#sys.path.insert(0, 'C:\\Users\\juanf\\OneDrive\\Documents\\GitHub\\Learning-graphs-from-data\\')
#import utils.gl.src.gl_models as gl
from models.gcn_models import GCN_backend, SGCN_backend
import gl.src.gl_models as gl
#from geom_median.numpy import compute_geometric_median
from scipy.sparse import coo_matrix, hstack, vstack, find

from scipy.signal import medfilt2d
from skimage.segmentation import slic, watershed
from skimage.color import rgb2gray
from skimage.filters import sobel, threshold_otsu
import scipy.io as sio


def clip_by_iqr_rule(img):
    x = img.flatten()
    q1 = np.quantile(x,0.25)
    q3 = np.quantile(x,0.75)
    iqr = q3-q1
    img = np.minimum( img, q3 + 1.5*iqr)
    img = np.maximum(img, q1 - 1.5*iqr)
    # 
    return img

def apply_median_filter(img,num_channels):
    #ws = 5
    #ws = 7
    ws = 5
    if num_channels > 1:
        for l in range(num_channels):
            img[:,:,l] = medfilt2d(img[:,:,l],kernel_size=ws)
    else:
        img = medfilt2d(img,kernel_size=ws)
    return img

def normalize_by_range(img):
    img_range = np.max(img.flatten())-np.min(img.flatten())
    img = img/img_range
    return img

def bitemporalimg2pseudorgbimg(preeimg,posteimg):

    rows,cols = preeimg.shape[0], preeimg.shape[1]
    pseudo_rgb_img = np.zeros((rows,cols,3))

    if preeimg.ndim > 2:
        pseudo_rgb_img[:,:,0] = np.mean(preeimg, axis = 2)
    else:
        pseudo_rgb_img[:,:,0] = preeimg
    if posteimg.ndim > 2:
        pseudo_rgb_img[:,:,1] = np.mean(posteimg, axis = 2)
    else:
        pseudo_rgb_img[:,:,1] = posteimg

    #dataset['after_avg'] = np.mean(dataset['after'], axis = 2) if L2 > 1 else dataset['after']
    #dataset['before_avg'] = np.mean(dataset['before'], axis = 2) if L1 > 1 else dataset['before']

    # Create a pseudo rgb image for super pixel segmentation
    
    #if not 'after_avg' in dataset.keys():
    #    pseudo_rgb_img[:,:,0] = dataset['after']
    #else:
    #    pseudo_rgb_img[:,:,0] = dataset['after_avg']
    
    #if not 'before_avg' in dataset.keys():
    #    pseudo_rgb_img[:,:,1] = dataset['before']
    #else:
    #    pseudo_rgb_img[:,:,1] = dataset['before_avg']

    pseudo_rgb_img[:,:,2] = np.abs(pseudo_rgb_img[:,:,0]-pseudo_rgb_img[:,:,1])    
    pseudo_rgb_img[:,:,2] = normalize_by_range(np.abs(pseudo_rgb_img[:,:,0]-pseudo_rgb_img[:,:,1]))
    #pseudo_rgb_img[:,:,2] = pseudo_rgb_img[:,:,2] - np.min(pseudo_rgb_img[:,:,2])
    pseudo_rgb_img[:,:,0] = pseudo_rgb_img[:,:,0] - np.min(pseudo_rgb_img[:,:,0])
    pseudo_rgb_img[:,:,1] = pseudo_rgb_img[:,:,1] - np.min(pseudo_rgb_img[:,:,1])

    return pseudo_rgb_img

def get_shape(dataset):
    # Check dataset structure     
    if not isinstance(dataset,dict):
        raise ValueError('dataset must be a dict with keys in {"after","before"} and values set as either 2-d or 3-d numpy arrays.')
    rows = dataset['after'].shape[0]
    cols = dataset['after'].shape[1]
    # Check multichannel image dimensions
    if np.max(rows-dataset['before'].shape[0],cols-dataset['before'].shape[1]) > 0:
        raise ValueError('dataset["before"] and dataset["after"] must have the same shape along first two dimensions.')
    c1, c2 = 1, 1
    if dataset['after'].ndim > 2:
        c2 = dataset['after'].shape[2]       
    if dataset['before'].ndim > 2:               
        c1 = dataset['before'].shape[2] 
    return rows, cols, c1, c2

def prepro_pipeline(dataset,n_spixels,context_radius,prepro_params):

    if not 'sp_method' in prepro_params.keys():
        raise ValueError("Make sure the key 'sp_method' in prepro_params exist, and takes values in ['slic','watershed'].")
    
    if not 'iqr_clipping' in prepro_params.keys():
        raise ValueError("Make sure the Key 'iqr_clipping' in prepro_params exist, and has Value of either True or False.")

    # Extract shape of bi-temporal satellite images stored in dataset
    rows, cols, c1, c2 = get_shape(dataset)
    
    # Convert images to float32 type
    dataset['after'] = dataset['after'].astype(np.float32)
    dataset['before'] = dataset['before'].astype(np.float32)

    # Filter out possible small support intensity outliers
    dataset['after'] = apply_median_filter(dataset['after'],num_channels=c2)
    dataset['before'] = apply_median_filter(dataset['before'],num_channels=c1)

    # Clip intensity values outside [med - 1.5*IQR, med + 1.5*IQR]
    if prepro_params['iqr_clipping']:
        dataset['after'] = clip_by_iqr_rule(dataset['after'])
        dataset['before'] = clip_by_iqr_rule(dataset['before'])

    dataset['after'] = normalize_by_range(dataset['after'])
    dataset['before'] = normalize_by_range(dataset['before'])

    # Create pseuso RGB image
    pseudo_rgb_img = bitemporalimg2pseudorgbimg(dataset['before'],dataset['after'])
    #pseudo_rgb_img = dataset['after'] #np.empty(shape=(rows,cols,3))
    #pseudo_rgb_img[:,:,0] = dataset['after']
    #pseudo_rgb_img[:,:,1] = dataset['after']
    #pseudo_rgb_img[:,:,2] = dataset['after']
    #pseudo_rgb_img[:,:,2] = normalize(pseudo_rgb_img[:,:,2],1)
    #pseudo_rgb_img[:,:,2] = pseudo_rgb_img[:,:,2]/np.max(pseudo_rgb_img[:,:,2].flatten())

    if not prepro_params['sp_method'] in ['slic','watershed']: 
        raise ValueError("sp_method must be a string in the list ['slic','wateshed']")
    
    if prepro_params['sp_method'] == 'slic':
        #segments = slic(pseudo_rgb_img, n_segments=n_spixels, compactness=15, sigma=1,
        #                start_label=1, channel_axis=2)
        segments = slic(pseudo_rgb_img, n_segments=n_spixels, compactness=15, sigma=1,
                        start_label=1, channel_axis=2)
        #segments1 = slic(pseudo_rgb_img[:,:,0], n_segments=n_spixels, compactness=15, sigma=0.1,
        #                start_label=1)
        #segments2 = slic(pseudo_rgb_img[:,:,1], n_segments=n_spixels, compactness=15, sigma=0.1,
        #                start_label=1)                        
        #n_spixels1 = len(np.unique(segments1.flatten()))
        #segments = (n_spixels+1)*np.ones(segments2.shape).astype(int) #np.zeros((rows,cols,n_spixels1)).astype(bool)
        #sp_pairs = {} # stores
        #for i in range(n_spixels1):
        #    mask_sp_i = segments1==(i+1) 
        #    vals, counts = np.unique(segments2[mask_sp_i],return_counts=True)
        #    sp_pairs[str(i+1)] = vals[np.argmax(counts)]
        #    segments[np.logical_and(mask_sp_i,(segments2==sp_pairs[str(i+1)]))] = sp_pairs[str(i+1)]
    elif prepro_params['sp_method']  == 'watershed': 
        gradient = sobel(rgb2gray(pseudo_rgb_img))
        segments = watershed(gradient, markers=n_spixels, compactness=0.001)
        
    # Generate node features X and one-hot encoded labels Y
    if not 'robust_avg_signals' in prepro_params.keys():
        prepro_params['robust_avg_signals'] = False
    
    # 
    if not 'position_signals' in prepro_params.keys():
        prepro_params['position_signals'] = False


    output_vars = img_2_node_features_matrix((dataset['before']/c1, dataset['after']/c2), 
                                                         dataset['gt'], segments, context_radius,
                                                         prepro_params['robust_avg_signals'],
                                                         prepro_params['position_signals'])
    
    # TODO: simplify this by outputing only output_vars, but make sure associated scripts are
    # modified accordingly

    if len(output_vars) == 3:
        X_mean, X, F = output_vars
        return X_mean, X, F, segments
    else: # len(output_vars) == 4
        X_mean, X, X_pos, F = output_vars
        return X_mean, X, X_pos, F, segments

def spixels_upsampling(f_hat, segments):

    if len(f_hat) < np.max(segments):
        segments[segments==np.max(segments)] = len(f_hat)+1
        f_hat = np.vstack((f_hat.reshape((len(f_hat),1)),0))
    #size = segments.shape
    (n1,n2) = segments.shape
    segments = segments.flatten()
    if np.min(segments)==1:
        segments = segments-1  
    upsampled_image = f_hat[segments]
    return upsampled_image.reshape((n1,n2))

def img_2_node_labels(gt,segments,R):
    H,W = gt.shape

    if not isinstance(segments,list):
        number_of_superpixels = len(np.unique(segments))
        gt_padded =np.pad(gt, pad_width = R, mode = 'constant', constant_values = -10)    
        segments_padded =np.pad(segments, pad_width = R, mode = 'constant', constant_values = -10)
    Y = np.zeros((number_of_superpixels,2))

    for i in range(number_of_superpixels):
        if not isinstance(segments,list):        
            spixel_i_mask = segments_padded == i+1
        else:
            y,x = segments[i]
            spixel_i_mask = np.zeros((H,W),dtype=bool)
            spixel_i_mask[y-R:y+R+1,x-R:x+R+1] = True
        if np.sum(gt_padded[spixel_i_mask],axis=0) > np.sum(1-gt_padded[spixel_i_mask],axis=0):
            Y[i,:] = [1,0]
        else:
            Y[i,:] = [0,1]    
    return Y

def img_2_node_features_matrix(img, gt, segments, R, robust_avg_flag, position_signals=False):

    """ segments: segmented image, or list of tuples (y,x) positions""" 

    
    H,W,C1 = img[0].shape if img[0].ndim>2 else (img[0].shape[0],img[0].shape[1], 1)    
    H,W,C2 = img[1].shape if img[1].ndim>2 else (img[1].shape[0],img[1].shape[1], 1)
    C = C1+C2
    img = np.dstack(img)

    if not isinstance(segments,list): 

        vals, counts = np.unique(segments, return_counts=True)

        number_of_superpixels = len(vals)

        img_padded = np.zeros((H+2*R,W+2*R,C))
        for i in range(C):
            img_padded[:,:,i] = np.pad(img[:,:,i], pad_width = R, mode = 'symmetric')
        segments_padded =np.pad(segments, pad_width = R, mode = 'constant', constant_values = -10)
        gt_padded =np.pad(gt, pad_width = R, mode = 'constant', constant_values = -10)
    else:
        # Check for patches indexed by entries of segments to be inside image boundaries
        filter = np.zeros((len(segments),1),dtype=bool)
        for i in range(len(segments)):
            y, x = segments[i]
            if (y-R >= 0 and y+R <= H) and (x-R >= 0 and x+R <= W):
                filter[i] = True 
        # keep only patches indide image boundaries
        segments = segments[filter.squeeze()]
        number_of_superpixels = len(segments)

        img_padded = img
        gt_padded  = gt

    # Allocate memory for average and feature signals on V
    X_context = np.zeros((number_of_superpixels,C*(2*R+1)**2))
    X_mean = np.zeros((number_of_superpixels,C))

    # Allocate memory for superpixels' (x,y) location on V
    X_pos = np.zeros((number_of_superpixels,2))
    Y = np.zeros((number_of_superpixels,2))

    #robust_avg = False
    if not robust_avg_flag:
        avg_func = lambda x : np.mean(x,axis=0)
        #X_mean[i,:] = np.mean(img_padded[spixel_i_mask,:],axis=0)
    else:
        avg_func = lambda x : np.median(x,axis=0)
        #X_mean[i,:] = np.median(img_padded[spixel_i_mask,:],axis=0)


    for i in range(number_of_superpixels):

        if not isinstance(segments,list):        
            spixel_i_mask = segments_padded == i+1
        else:
            y,x = segments[i]
            spixel_i_mask = np.zeros((H,W),dtype=bool)
            spixel_i_mask[y-R:y+R+1,x-R:x+R+1] = True


        X_mean[i,:] = avg_func(img_padded[spixel_i_mask,:])

        #    superpixel_matrix = img_padded[spixel_i_mask,:] # np.array of shape (num_of_pixels on spixel_i,C)
        #    points = [superpixel_matrix[i,:] for i in range(superpixel_matrix.shape[0])]
        #    X_mean[i,:] = compute_geometric_median(points).median

        #if context:
        subscripts = np.argwhere(spixel_i_mask)
        #idx_spixel = np.argmin(np.sum((subscripts - np.mean(subscripts,axis=0))**2,axis=1))
        #try:
        idx_spixel = np.argmin(np.sum((img_padded[spixel_i_mask,:] - X_mean[i,:].reshape((1,C)))**2,axis=1)) 
        #except:
        #    print('Except block active: choose context window center uniformly at random.')
        #    idx_spixel = np.sort(np.random.rand(subscripts.shape[:,0],1).flatten())[0]

        x0 = subscripts[idx_spixel,1]
        y0 = subscripts[idx_spixel,0]
        X_pos[i,:] = np.array([x0,y0])
        #img_patch = np.reshape(img_padded[y0-R:y0+R+1,x0-R:x0+R+1,:],(C,(2*R+1)**2))
        #img_patch = (img_patch - np.reshape(np.mean(img_patch,axis=1),(2,1)))/np.reshape(np.std(img_patch,axis=1),(2,1))

        context_feature = np.reshape(img_padded[y0-R:y0+R+1,x0-R:x0+R+1,:],(1,C*(2*R+1)**2))

        #context_feature = context_feature/np.max(context_feature)

        X_context[i,:] = context_feature



        if np.sum(gt_padded[spixel_i_mask],axis=0) > np.sum(1-gt_padded[spixel_i_mask],axis=0):
            Y[i,:] = [1,0]
        else:
            Y[i,:] = [0,1]

    #X_context = X_context/np.max(X_context,axis=0).reshape((1,C*(2*R+1)**2))
    #X_context = X_context-np.mean(X_context,axis=0).reshape((1,C*(2*R+1)**2))
    #X_context = X_context/np.std(X_context,axis=0).reshape((1,C*(2*R+1)**2))

    #X_mean = X_mean-np.mean(X_mean,axis=0).reshape((1,C))
    #X_mean = X_mean/np.std(X_mean,axis=0).reshape((1,C))

    
    #X_context = X_context/np.max(X_mean.flatten())
    #X_mean = X_mean/np.max(X_mean.flatten())
    
    #X_context_norm = np.sqrt(np.sum(X_context**2,axis=0))
    #X_context_norm[X_context_norm==0] = 1
    #X_context = X_context/X_context_norm.reshape((1,C*(2*R+1)**2))
    n = number_of_superpixels
    X_mean = (X_mean[:,0:C1].reshape((n,C1)),X_mean[:,C1:C1+C2].reshape((n,C2)))
    X_context = (X_context[:,0:C1*(2*R+1)**2].reshape((n,C1*(2*R+1)**2)),X_context[:,C1*(2*R+1)**2:C*(2*R+1)**2].reshape((n,C2*(2*R+1)**2)))
    
    if position_signals:
        return X_mean, X_context, X_pos, Y 
    else: 
        return X_mean, X_context, Y 


def construct_sampling_set(m, n, sampling_params = {'seed': 0, 'type': 'random'}, matrix_form = False):
    """ constructs a binary sampling matrix S of shape (nclasses*round(p*min(card(class_i))),n).
    Parameters:
    m (int): number of labeled examples per class
    n (int): number of examples
    y, integer encoded class labels
    """
    #seed = sampling_params['seed']
    #np.random.seed(sampling_params['seed'])
    rng = np.random.RandomState(sampling_params['seed'])

    #if sampling_params['type'] == 'random-class-dependent':
    if sampling_params['type'] == 'stratified_random_sampling':
        if not 'labels' in sampling_params.keys():
            raise ValueError('sampling_params["labels"] does not exists. ')

        class_labels, class_cards = np.unique(sampling_params["labels"],return_counts=True)
        nclasses = len(class_labels)
        #m = round(label_rate*n/nclasses)

        sample_idx = []
        for c in range(nclasses):
            c_class_idx = np.argwhere(sampling_params["labels"]==class_labels[c]).squeeze()
            c_class_card = class_cards[c]
            
            # Include all samples from classes with less than m elements in them
            if m >= c_class_card:
                m_c = int(c_class_card)
            else:
                m_c = m
            
            #random_perm_c_idx = np.argsort(np.random.rand(c_class_card,1),axis=0).squeeze()
            if c_class_card > 1:
                random_perm_c_idx = np.argsort(rng.uniform(0,1,size=(c_class_card,1)),axis=0).squeeze()
                random_perm_c_idx = random_perm_c_idx[0:m_c]
                for i in range(len(random_perm_c_idx)):
                    sample_idx.append(c_class_idx[random_perm_c_idx[i]])
            else:
                sample_idx.append(c_class_idx)
            #if c > 0:
            #    sample_idx = np.concatenate((sample_idx.squeeze(), c_class_idx[random_perm_c_idx].squeeze()))
            #else:
            #    sample_idx = c_class_idx[random_perm_c_idx].squeeze()
    elif sampling_params['type'] == 'blue_noise_sampling': # random sampling of nodes
        if not 'W' in sampling_params.keys():
            raise ValueError('Adjacency matrix of shape (n,n) must provided in sampling_params["W"].')
        #m = round(label_rate*n)
        maxIt = 1000
        sampling_pattern = blue_noise_sampling(sampling_params["W"],m,maxIt,sampling_params['seed'])
        sample_idx = np.arange(0,n)
        sample_idx = sample_idx[sampling_pattern.astype(bool).squeeze()]
    else: # random sampling
        #m = round(label_rate*n)
        sample_idx = np.argsort(rng.uniform(0,1,size=(n,1)),axis = 0)[0:m]

    if matrix_form:
        #print(sample_idx)
        I = sp.sparse.eye(n)
        return I.tocsr()[np.asarray(sample_idx).squeeze(),:] 
    else:
        return np.asarray(sample_idx).squeeze()

def generate_train_dataset(m,n,sampling_params):

    idx_train = construct_sampling_set(m, n, sampling_params, matrix_form = False).astype(int) 
    y_train = sampling_params['labels'][idx_train]
    idx_test_tmp= np.ones((n,1),dtype=bool).flatten()
    idx_test_tmp[idx_train] = False 
    idx_test = np.arange(0,n)
    idx_test = idx_test[idx_test_tmp]

    return y_train, idx_train

def generate_validation_dataset(idx_train,m,n,sampling_params):
    #idx_train = construct_sampling_set(m, n, sampling_params, matrix_form = False).astype(int) 
    #y_train = sampling_params['labels'][idx_train]
    idx_remaining_tmp= np.ones((n,1),dtype=bool).flatten()
    idx_remaining_tmp[idx_train] = False 
    idx_remaining = np.arange(0,n)
    idx_remaining = idx_remaining[idx_remaining_tmp]

    sampling_params['labels'] = sampling_params['labels'][idx_remaining_tmp]
    idx_val_reduced_labels = construct_sampling_set(m, n, sampling_params, matrix_form = False).astype(int) 

    idx_val = idx_remaining[idx_val_reduced_labels]
    y_val = sampling_params['labels'][idx_val_reduced_labels]

    return y_val, idx_val

def add_random_flip_noise(y,m_noise,seed=34):
    """ corrupts a number of binary labels, defined by m_noise, uniformly at random by fliping their values."""
    if len(np.unique(y)) > 2:
        raise ValueError("y must not contain non-binary labels. That is, for all i, y[i] in {0,1}.")
    #np.random.seed(seed)
    rng = np.random.RandomState(seed)
    random_sel = np.argsort(rng.uniform(0,1,size = (y.size,1)),axis=0).squeeze()
    #random_sel = np.argsort(np.random.rand(y.size,1),axis=0).squeeze()
    if m_noise > 0:
        y[random_sel[0:m_noise]] = np.logical_xor(y[random_sel[0:m_noise]],np.ones((1,m_noise)))
    return y

def construct_adj_matrix(X, gl_params = {'model': 'Kalofolias', 'k': 1, 'knn_edge_cons': False, 'k_': 2}):

    """ Constructs an n-by-n graph adjacency matrix based on the n-by-d data matrix X using
    a structure learning model defined by gl_params.
    
    Parameters:
    X (np.array or tuple with np.array): n-by-d data matrix, or tuple of data matrices, denoting the d-dimensional
     features on a graph with n nodes.
    gl_params (dict): dictionary with structure learning model parameters.
    
    Returns:
    W (np.array): n-by-n adjacency matrix inferred from X
    dd_params (dict): dictionary with associated hyperparameters
    """

    dd_params = {}
    if isinstance(X,tuple): # if true, X must be a tuple with two elements, 
    # where each element is an np.array with n rows and a number of columns.
        if 'fusion_rule' in gl_params.keys():
            if gl_params['fusion_rule'] in ['sum']:
                #z = sp_sd.pdist(np.hstack(X), 'euclidean')**2
                z_1 = sp_sd.pdist(X[0], 'euclidean')
                sigma1 = np.mean(z_1)
                z_1 = z_1/sigma1
                z_2 = sp_sd.pdist(X[1], 'euclidean')
                sigma2 = np.mean(z_2)
                z_2 = z_2/sigma2
                z = z_1**2 + z_2**2
                dd_params = {'sigma1':sigma1,'sigma2':sigma2}
            elif gl_params['fusion_rule'] in ['max']:
                z_1 = sp_sd.pdist(X[0], 'euclidean')**2
                sigma1_sq = np.mean(z_1)
                z_2 = sp_sd.pdist(X[1], 'euclidean')**2
                sigma2_sq = np.mean(z_2)
                z = np.maximum(z_1/sigma1_sq,z_2/sigma2_sq)
                dd_params = {'sigma1':np.sqrt(sigma1_sq),'sigma2':np.sqrt(sigma2_sq)}
            del z_1, z_2
    else:
        # Construct pair-wise distance matrix from X
        d = X.shape[1]
        #z = (np.sqrt(d)*sp_sd.pdist(X, 'chebyshev'))**2
        #z = sp_sd.pdist(X, 'chebyshev')
        z = sp_sd.pdist(X, 'euclidean')**2
        sigma_sq = np.mean(z)/(3**2)

        z = z/sigma_sq
        
        
        dd_params['sigma']= np.sqrt(sigma_sq)

    Z = sp_sd.squareform(z) # turns the condensed form into a n by n distance matrix

    if 'tol' in gl_params:
        params = {'tol': gl_params['tol']} 
    else:
        params = {'tol': 1e-5}    
    
    if 'maxit' in gl_params:
        params['maxit'] = gl_params['maxit']
    else:
        params['maxit'] = 10000
    params['verbosity'] = 3
    params['nargout'] = 1

    #params['w_0'] = np.zeros((m,m))
    #params['c'] = 1
    
    # Define graph learning hyperparameters 
    if gl_params['model'] in ['Gaussian','Kalofolias']:   
        k = gl_params['k']
        knn_edge_cons = gl_params['knn_edge_cons'] if gl_params['model']=='Kalofolias' else True
        k_ = gl_params['k_'] if gl_params['model']=='Kalofolias' else k
        if knn_edge_cons or (gl_params['model'] in ['Gaussian']):
            edge_index = knn_graph(torch.Tensor(np.hstack(X) if isinstance(X,tuple) else X), k_, loop=False)
            edge_index = edge_index.numpy()
            params['fix_zeros'] = True
            params['edge_mask'] = sp.sparse.coo_matrix((np.ones((edge_index[0].shape)).squeeze(), (edge_index[0],edge_index[1])), shape=Z.shape) #np.zeros(Z.shape) #np.reshape(np.array([1,1,1,1,0,0,0,1,0,0,0,0,0,0,0,0]),(16,1))
            params['edge_mask'] = params['edge_mask'].maximum(params['edge_mask'].T)


    a = 1
    b = 1
    if gl_params['model'] == 'Kalofolias':
        theta = gl.estimate_theta(Z,k)
        W = gl.gsp_learn_graph_log_degrees(theta*Z,a,b,params)
        W = W.maximum(0)
        #W[W<0] = 0
        W = W/W.max() #np.max(W[W>0])  
        #list_of_nodes = sp_sparse.csgraph.depth_first_order(W, 0, directed = False, return_predecessors = False)
        dd_params['theta'] = theta
        #return W, theta, sigma1, sigma2
    elif gl_params['model'] == 'Gaussian':
        W = sp.sparse.coo_matrix((np.exp(-0.5*Z[edge_index[0],edge_index[1]]), (edge_index[0],edge_index[1])), shape=Z.shape).tocsr()
        #W[edge_index[0],edge_index[1]] = np.exp(-0.5*W[edge_index[0],edge_index[1]])
        W = W.maximum(W.T)
        W = W/W.max()
    elif gl_params['model'] == 'MST_with_binary_weights': #'mst_with_gaussian_weights':
        
        # Add a small weight to all edges to ensure the MST on Z is connected
        #epsilon = 1e-5
        #Z = Z + epsilon
        
        # Compute the MST
        W = sp_sparse.csgraph.minimum_spanning_tree(Z, overwrite=False)
        sigma_sq = (np.median(W.max(axis=1).toarray()) / 3)
        I, J, V = sp_sparse.find(W)
        W_binary = sp_sparse.csr_matrix((np.ones_like(V), (I, J)), shape=W.shape)
        #W[I, J] = 1  # Binary weights
        W_binary = W_binary.maximum(W_binary.T)  # Ensure symmetry
        #list_of_nodes = sp_sparse.csgraph.depth_first_order(W, 0, directed=False, return_predecessors=False)
        #W = W / W.max()  # Normalize
        return W_binary, dd_params
    return W, dd_params

def structure_learning_pipeline(X_mean,mode,gl_params,log_filename,output_dir):


    if isinstance(X_mean,tuple):
        n, d1 = X_mean[0].shape
        d = d1 + X_mean[1].shape[1]
    else: # 2d ndarray
        n, d = X_mean.shape

    # Compute a graph defined by params['gl_params'] from the avg pre and post event signals
    W, outparams = construct_adj_matrix(X_mean, gl_params)

    if mode == 'unsupervised':
        # Compute a knn graph with Gaussian weights from pre-event avg. signals.
        # TODO: understand better the effect of choosing this graph
        gl_params_1 = {'model': 'Gaussian', # or 'Gaussian'
                    'k': gl_params['k']
                    }
        if not 'X_pos' in gl_params.keys():
            W1, outparams1 = construct_adj_matrix(X_mean[0], gl_params_1)  
        else:
            # Modulate Gaussian weights with superpixel position information
            gl_params_1['fusion_rule'] = 'sum'
            W1, outparams1 = construct_adj_matrix((X_mean[0],gl_params['X_pos']), gl_params_1)  

    output_path = os.path.join(output_dir,log_filename)
    rfile = open(output_path, 'w')
    rfile.write('\nGraph learning params: \n')   
    rfile.write('-Number of nodes: ' + str(n) + '\n' )
    if mode == 'unsupervised':
        rfile.write('\nGraph G1: \n')   
        rfile.write('-Method: ' + gl_params_1['model'] + '\n' ) 
        rfile.write('-Number of nearest neighbors: ' + str(gl_params_1['k']) + '\n' )
        #rfile.write('-Fusion rule: ' + str(gl_params['fusion_rule']) + '\n' )
    rfile.write('\nGraph G: \n')   
    rfile.write('-Method: ' + gl_params['model'] + '\n' ) 
    rfile.write('-Number of nearest neighbors: ' + str(gl_params['k']) + '\n' )
    rfile.write('-Tol: ' + str(gl_params['tol']) + '\n' )
    rfile.write('-Fusion rule: ' + str(gl_params['fusion_rule']) + '\n' )
    rfile.close()

    if mode == 'unsupervised':
        return W, W1, outparams, outparams1
    else:
        return W, outparams 


def blue_noise_sampling(W,s,maxIt,seed):

    n = W.shape[0]

    if n == s:
        return np.ones((n,1))
    # Compute geodesic distance matrix
    K = sp.sparse.csgraph.shortest_path(csgraph=W, directed=False, unweighted=False, method = 'D')
    # 
    sigma = np.mean(K.flatten())#-np.std(K.flatten())
    K = np.exp(-(K*K)/(2*(sigma**2)))

    np.random.seed(seed)
    rnd_idx = np.argsort(np.random.rand(n,1),axis=0).squeeze().tolist()

    # Random pattern initialization
    sampling_pattern = np.zeros((n,1))
    sampling_pattern[rnd_idx[0:s]] = 1

    old_idx_tightest_cluster = 1
    old_idx_largest_void = 1
    idx_tightest_cluster = 0
    idx_largest_void = 0

    cntr = 0
    while (idx_largest_void != old_idx_tightest_cluster) or (idx_tightest_cluster!= old_idx_largest_void) or (cntr > maxIt):

        old_idx_tightest_cluster = idx_tightest_cluster
        old_idx_largest_void = idx_largest_void

        nodes_2_samples_density = np.sum(K[:,np.argwhere(sampling_pattern.squeeze()).squeeze()], axis = 1)

        idx_clusters = np.argwhere(sampling_pattern.squeeze())
        idx_voids = np.argwhere(1 - sampling_pattern.squeeze())

        idx_tightest_cluster =  idx_clusters[np.argmax(nodes_2_samples_density[idx_clusters],axis=0)].squeeze()
        idx_largest_void =  idx_voids[np.argmin(nodes_2_samples_density[idx_voids],axis=0)].squeeze()   

        # Swap tightest cluster sample with largest void sample (unselected node)

        sampling_pattern[idx_tightest_cluster] = 0
        sampling_pattern[idx_largest_void] = 1

        cntr = cntr + 1

    return sampling_pattern

def save_figure(path_to_file):
    cntr = 0
    while os.path.isfile(path_to_file):
        cntr = cntr + 1
        idx_start = path_to_file.find('_v_')
        path_to_file = path_to_file[0:idx_start] + '_v_' + str(cntr) + '.png' 
    
    plt.savefig(path_to_file, dpi=600, bbox_inches='tight')

def initialize_gcn_backend(model,type,seed):
    rng = np.random.RandomState(seed) #v001
    if isinstance(model,GCN_backend) or isinstance(model,SGCN_backend):
        for param_name, param in model.named_parameters():
            rows, cols = param.data.shape
            if 'conv1' in param_name:
                if type == 'Gaussian':
                    param.data = torch.from_numpy(rng.randn(rows,cols)/np.sqrt(cols)).float()
                elif type == 'Uniform':
                    param.data = torch.from_numpy(2*rng.rand(rows,cols)/np.sqrt(cols)-1/np.sqrt(cols)).float()
                elif type == 'Glorot':
                    init_range = np.sqrt(6.0/(rows+cols))
                    param.data = torch.from_numpy(2*rng.rand(rows,cols)*init_range - init_range).float()
                elif type == 'Id':
                    if cols < rows:
                        param.data = torch.from_numpy(np.vstack([np.eye(cols,cols), np.zeros((rows-cols,cols))])).float()
                    else:
                        param.data = torch.from_numpy(np.eye(cols,cols)).float()
            elif 'conv2' in param_name:
                param.data = torch.from_numpy(np.eye(cols,cols)).float()

def construct_subspace(W, param={'basis_params': ('RLP',2)}):
    """ returns a basis of size n by k associated with W.
    Parameters:
    W (sparse matrix): n by n adjacency matrix
    method (string): string of characters in 'sc', 'nystrom'
    """

    #_sq_inv = sp.sparse.spdiags(1/np.sqrt(np.sum(W,1)).squeeze(),0, W.shape[0], W.shape[1])
    #W = D_sq_inv @ (W @ D_sq_inv)

    n = W.shape[0]
    I = sp.sparse.eye(n)
    D = sp.sparse.spdiags(np.sum(W,1).squeeze(),0, W.shape[0], W.shape[1])
    L = D - W

    #D_sq_inv = sp.sparse.spdiags(np.power(np.sum(W,1),-0.5).squeeze(),0, W.shape[0], W.shape[1])
    #L = I - D_sq_inv @ (W @ D_sq_inv)

    epsilon = 1e-3
    L = L + epsilon*I

    n = W.shape[0]
    method_name, k = param['basis_params']

    if not 'gnn_init' in param.keys():
        param['gnn_init'] = 'Gaussian'
    if not param['gnn_init'] in ['Gaussian','Uniform','Glorot','Id']:
        raise ValueError("param['gnn_init'] must take values in ['Gaussian','Uniform','Glorot','Id']")

    if method_name == 'sc':
        #I = sp.sparse.eye(n)  
        SS, U = sp.linalg.eigh(L.toarray())
        #U = U @ np.diag(np.sqrt(1/(SS+1)))
        U = U[:,0:k]

    elif method_name == 'RLP':#'smoothness_prior':
        eps = 1e-3
        k0 = 0
        SS, U = sp.linalg.eigh(L.toarray())
        #U = U @ np.diag(np.sqrt(1/(SS+1)))
        U = U[:,k0:k] @ np.diag(np.sqrt(1/(SS[k0:k])))
    
    elif method_name == '2-GCN':
        X = torch.from_numpy(param['feature_matrix']).float()
        num_of_features = X.shape[1]
        hidden_chs = k
        n_classes = k
        model = GCN_backend(num_of_features,hidden_chs,n_classes)
        initialize_gcn_backend(model,type=param['gnn_init'],seed=1604)
        # Convert adjacency matrix to pytorch format 
        output = find(W)
        edge_indices =torch.tensor(np.vstack((output[0],output[1])),dtype =torch.int64)
        edge_weights = torch.tensor(output[2],dtype = torch.float32)
        U = model(X,edge_indices,edge_weights).detach().numpy()
        
        # add bias
        #c = np.mean(np.sqrt(np.sum(U**2,axis=0)))/np.sqrt(n)
        #U = np.hstack((c*np.ones((n,1)),U))
        #k = k+1       
        #U_norm = np.sqrt(np.sum(U**2,axis=0))
        #U_norm[U_norm<=1e-6] = 1
        #U = U/U_norm
        #g = np.sum(U * (L @ U),axis=0)
        #U = U @ (np.diag(1/(g + 1e-3)))


    elif method_name == '2-SGCN':
        X = torch.from_numpy(param['feature_matrix']).float()
        num_of_features = X.shape[1]
        hidden_chs = k
        n_classes = k
        model = SGCN_backend(num_of_features,n_classes,2)
        initialize_gcn_backend(model,type=param['gnn_init'],seed=1604)
        #initialize_model(model,type='Id',seed=1604)
        # Convert adjacency matrix to pytorch format 
        output = find(W)
        edge_indices =torch.tensor(np.vstack((output[0],output[1])),dtype =torch.int64)
        edge_weights = torch.tensor(output[2],dtype = torch.float32)
        U = model(X,edge_indices,edge_weights).detach().numpy()
        U = np.hstack((np.ones((n,1)),U))
        #U_norm = np.sqrt(np.sum(U**2,axis=0))
        #U_norm[U_norm<=1e-6] = 1
        #U = U/U_norm
        #g = np.sum(U * (L @ U),axis=0)
        #U = U @ (np.diag(1/(g + 1e-3)))
    elif method_name  == 'smoothness_prior_heat_kernel':
        eps = 5
        SS, U = sp.linalg.eigh(L.toarray())
        eps = -np.log(0.125)/SS[int(n/2)]
        #U = U @ np.diag(np.sqrt(1/(SS+1)))
        U = U[:,0:k] @ np.diag(np.exp(-eps*SS[0:k]))

    elif method_name  == 'nystrom':
        
        idx = np.arange(0,n)
        #maxIt = 1000
        #bn_sampling_pattern = gcd_utils.blue_noise_sampling(W,k,maxIt)
        #rnd_idx = np.argsort(-100*bn_sampling_pattern + np.random.rand(n,1),axis=0).squeeze().tolist()
        #np.random.seed(10)
        rnd_idx = np.argsort(np.random.rand(n,1),axis=0).squeeze().tolist()

        idx_p = idx[rnd_idx]    
        P = coo_matrix((np.ones((n,1)).squeeze(),(idx_p,np.arange(0,n))), shape=(n,n)).tocsr()
        W = P.T @ (W @ P)

        S_kxk, U_kxk = sp.linalg.eigh(W[0:k,0:k].toarray())
        S_kxk_inv = 1/S_kxk
        S_kxk_inv[S_kxk==0] = 0
        #  W[k:n,0:k] <=>  W[0:k,k:n].T    
        U = (P @ np.vstack((U_kxk, (W[k:n,0:k] @ ( U_kxk @ np.diag(S_kxk_inv))))))
        U = U/np.sqrt(np.sum(U**2,axis=0)).reshape((1,k))
        U = U @ np.diag(np.exp(S_kxk))

    if method_name  == 'sc':
        return U, SS
    else:
        return U
    
# Quality metrics
def kappa_coeff(y,y_hat):
    #from sklearn.metrics import confusion_matrix
    #TN, FP, FN, TP = confusion_matrix(y, y_hat).ravel()

    confusion_img = np.zeros(y.shape)
    # vector of true positives
    tmp = np.logical_and(y,y_hat)
    confusion_img[tmp] = 3
    # count true positives
    TP = np.sum(tmp.astype(np.float32))
    # vector of false positives
    tmp = np.logical_and(np.logical_not(y),y_hat)
    confusion_img[tmp] = 2
    FP = np.sum(tmp.astype(np.float32))
    # vector of false negatives 
    tmp = np.logical_and(y,np.logical_not(y_hat))
    confusion_img[tmp] = 1
    FN = np.sum(tmp.astype(np.float32))

    # vector of true negatives 
    tmp = np.logical_and(np.logical_not(y),np.logical_not(y_hat))
    confusion_img[tmp] = 0
    TN = np.sum(tmp.astype(np.float32))

    decoding_map = { 3 : 'TP', 2: 'FP', 1: 'FN', 0: 'TN'}

    
    kappa = 2*((TP*TN)-(FN*FP))/((TP+FP)*(FP+TN)+(TP+FN)*(FN+TN))

    tpr = TP/(TP + FN) # recall
    precision = TP/(TP+FP) # precision
    #tnr = TN/(TN + FP)
    return kappa, tpr, precision, {'confusion_img': confusion_img,'decoding_map': decoding_map}

import matplotlib.patches as mpatches
def plot_confusion_img_on_ax(conf_img_dict,n1,n2,ax,add_legend=True,default_loc = False):
    
    # plot confusion change map
    #color_encoding_map = {'TP': np.array([42, 247, 10])/255,
    #                      'FP': np.array([10, 18, 247])/255,
    #                      'FN': np.array([247, 10, 22])/255,
    #                      'TN': np.array([255, 255, 255])/255}
    
    color_encoding_map = {'TP': np.array([253, 231, 37])/255,
                          'FP': np.array([10, 18, 247])/255,
                          'FN': np.array([247, 10, 22])/255,
                          'TN': np.array([68, 1, 84])/255}
    
    tmp = conf_img_dict['confusion_img'].flatten()
    conf_img = [color_encoding_map[conf_img_dict['decoding_map'][tmp[i]]] for i in range(len(tmp))]
    conf_img = np.reshape(np.array(conf_img),(n1,n2,3))
    
    #fig, ax = plt.subplots()
    #ax = fig.get_axes()
    ax.imshow(conf_img)
    fontsize = 4
    # Put those patched as legend-handles into the legend
    if add_legend:
        #fig, axs = plt.subplots(1,len(alg_list), figsize=(10, 10), sharex=True, sharey=True)
        patches = [ mpatches.Patch(color=color_encoding_map[i], label=i, edgecolor='black') for i in ['TP','FP','FN','TN'] ]
        #ax.legend(handles=patches, bbox_to_anchor=(1.25, 1.5), loc='upper center', borderaxespad=0., fontsize = 'small',ncol=4)        
        if not default_loc:
            ax.legend(handles=patches,bbox_to_anchor=(-0.5, 0.5), loc= 'center left', borderaxespad=0.,fontsize='small')#,ncol=4)        
        else:
            ax.legend(handles=patches,loc= 'best',bbox_to_anchor=(1.15, 1.0), borderaxespad=0.,fontsize='small')#,ncol=4)

    #return tmp_img_name
    #return fig, ax   
    # 
         
    




def compute_performance_metrics(gt_map, score_map,ref_fpr):
    
    fpr, tpr, thresholds = metrics.roc_curve(gt_map.flatten(), score_map.flatten(), pos_label=1)
    
    auc_value = metrics.auc(fpr,tpr)
    #ref_fpr = 0.05
    idx_ref_fpr = np.argmin(np.abs(fpr-ref_fpr))

    tpr_ref_hat = tpr[idx_ref_fpr]
    fpr_ref_hat = fpr[idx_ref_fpr]

    c_hat = lambda thr: np.digitize(score_map, bins=[thr])
    f1_score_value = metrics.f1_score(gt_map.flatten(),c_hat(thresholds[idx_ref_fpr]).flatten())
    kappa_value = metrics.cohen_kappa_score(gt_map.flatten(),c_hat(thresholds[idx_ref_fpr]).flatten())

    #kappa_list = [metrics.cohen_kappa_score(gt_map.flatten(),c_hat(thr).flatten()) for thr in thresholds ]
    #idx_max = np.argmax(np.array(kappa_list))
    #kappa_max = kappa_list[idx_max]
    #thrs_max = thresholds[idx_max]

    metrics_dict = {'roc_curve': {'tpr':tpr,'fpr':fpr},
                    'metrics_labels': ['AUC','F1-SCORE','KAPPA','TPR_REF','FPR_REF'],
                    'metrics': [auc_value, f1_score_value, kappa_value, tpr_ref_hat,fpr_ref_hat],
                    'threshold': thresholds[idx_ref_fpr]
                    #'thrs_max': thrs_max,
                    #'kappa_max':kappa_max
                    }
    return metrics_dict 

import scipy.io as sio
def dataset_loader(datasets_dir,dataset_name):

    if not os.path.isdir(datasets_dir):
        raise ValueError("datasets_dir does not exists. Enter a valid directory's path.")
    
    dataset_filename = {"Alaska": "Alaska_L5.mat",
                        "California": "California.mat",
                        "Atlantico": "Atlantico_dataset.mat",
                        "Mulargia": "Mulargia_L5.mat",
                        "Toulouse_1": "Toulouse_dataset.mat",                        
                        "Shuguang": "Shuguang.mat",
                        "Gloucester_2": "Gloucester_2_opt_sar.mat",                          
                        "Palmas_Volcano": "Palmas_Volcano.mat",                      
                        "Kaliveli_Lake": "Kaliveli_Lake.mat",                        
                        "Machadinho_dOeste": "Machadinho_dOeste_L5.mat",
                        "Bastrop": "Bastrop_L5_ALI.mat",                        
                        "Canada": "Canada_L8.mat",
                        "Gloucester_1": "Gloucester_1_Spot.mat",
                        "Higashimatsushima": "Higashimatsushima.mat",
                        "Katios": "Katios_dataset.mat",
                        "Omodeo": "Omodeo_L8.mat",
                        "San_Francisco": "SF_dataset.mat",
                        "Toulouse_2": "Toulouse_2.mat",
                        "Town_Island": "Town_Island.mat",
                        "Watari_Iwanuma": "Watari_Iwanuma.mat",
                        "Wenchuan": "Wenchuan_dataset.mat"
                    }
    #dataset_filename = {'Alaska': 'Alaska_L5', #'Alaska_full_dataset.mat',
    #                    'California': 'California_full_dataset.mat',
    #                    'Atlantico': 'Atlantico_dataset.mat',
    #                    'Mulargia': 'Mulargia_L5.mat',
    #                    'Toulouse': 'Toulouse_2.mat',
    #                    'Shuguang': 'Shuguang.mat',
    #                    'Gloucester_2_opt_sar':'Gloucester_2_opt_sar.mat',
    #                    'Kaliveli_Lake':'Kaliveli_Lake.mat',
    #                    'Palmas_Volcano': 'Palmas_Volcano.mat',
    #                    'Machadinho_dOeste':'Machadinho_dOeste_L5.mat'}
    
    if dataset_name in dataset_filename.keys():
        dataset_path = os.path.join(datasets_dir,dataset_filename[dataset_name])
    else:
        raise ValueError(dataset_name + 'must belong in' + dataset_filename.keys())

    dataset = sio.loadmat(dataset_path)
    dataset_idx = list(dataset_filename.keys()).index(dataset_name)

    # add some metadata
    dataset['metadata'] = {'name':dataset_name}
    """    if dataset_idx == 0:
            dataset['metadata'] = {'name': 'Alaska', 
                                'pre-event captured by': 'Landsat-5 TM',
                                'post-event captured by': 'Landsat-5 TM'}
        elif dataset_idx == 1: 
            dataset['metadata'] = {'name': 'California', 
                                'pre-event captured by': 'Lansat-8',
                                'post-event captured by': 'Sentinel-1A'}
        elif dataset_idx == 2: 
            dataset['metadata'] = {'name': 'Atlantico', 
                                'pre-event captured by': 'ALOS/PALSAR',
                                'post-event captured by': 'ALOS/PALSAR'}
        elif dataset_idx == 3:
            dataset['metadata'] = {'name': 'Mulargia', 
                                'pre-event captured by': 'Landsat-5 TM',
                                'post-event captured by': 'Landsat-5 TM'}
        elif dataset_idx == 4:
            dataset['metadata'] = {'name': 'Toulouse_1', 
                                'pre-event captured by': 'TerraSAR-X',
                                'post-event captured by': 'Pleiades'}
        elif dataset_idx == 5:        
            dataset['metadata'] = {'name': 'Shuguang', 
                                'pre-event captured by': 'Radarsat-2',
                                'post-event captured by': 'Google Earth'}
            
        elif dataset_idx == 6:        
            dataset['metadata'] = {'name': 'Gloucester_2', 
                                'pre-event captured by': 'Quickbird 02',
                                'post-event captured by': 'TerraSAR-X',
                                'cutoff_idx_E90_n2400': 9} 
        elif dataset_idx == 7:        
            dataset['metadata'] = {'name': 'Kaliveli_Lake', 
                                'pre-event captured by': 'S2A',
                                'post-event captured by': 'S1A',
                                'cutoff_idx_E90_n2400': 203}
        elif dataset_idx == 8:        
            dataset['metadata'] = {'name': 'Palmas_Volcano', 
                                'pre-event captured by': 'ALOS/P',
                                'post-event captured by': 'S2A',
                                'cutoff_idx_E90_n2400': 390}
        elif dataset_idx == 9:
            dataset['metadata'] = {'name': 'Machadinho_dOeste', 
                                'pre-event captured by': 'Landsat-5 TM',
                                'post-event captured by': 'Landsat-5 TM'}
        else:
            dataset['metadata'] = {'name':dataset_name}"""
        
    return dataset
