import numpy as np
import pandas as pd
from skimage.segmentation import mark_boundaries
import pickle
import matplotlib.pyplot as plt
import scipy.io as sio

import utils.gcd_utils as gcd_utils
import models.gcd_models as gcd_models
import models.gcn_models as gcn_models
import models.gcd_vi_models as gcd_vi_models
import os


def create_and_return_proposed_alg_list(basis_name_list, ball_type_list):

    # Create list with possible proposed algorithm configurations
    prop_alg_name = lambda basis_name, norm: 'Proposed (' + basis_name + ',' + norm + ')'
    proposed_alg_list = []
    #for basis_name in ['RLP','2-GCN','2-SGCN']:
    for basis_name in basis_name_list:
        if not basis_name in ['RLP','2-GCN','2-SGCN']:
            raise ValueError("basis_name_list must consists of elements in ['RLP','2-GCN','2-SGCN']")
        for norm in ball_type_list:
            if not norm in ['1','2','inf']:
                raise ValueError("ball_type_list must consists of elements in ['1','2','inf']")
        #for norm in ['2','1','inf']:
            proposed_alg_list.append(prop_alg_name(basis_name,norm))
    return proposed_alg_list

def parse_alg_params(alg_name, y, n_input_units, n_hidden_units, n_layers, wd, maxit, lr, theta0):
        
        alg_param = {'name': alg_name, 
                      'n_classes': len(np.unique(y))}
        if alg_name in ['2-GCN','2-SGCN']:
            alg_param['n_hidden_units'] =  n_hidden_units
            alg_param['n_layers'] =  n_layers
            alg_param['ss_params'] = {'type': 'without',                                       
                                       'weight_decay': wd, 
                                       'maxit': maxit, 'lr': lr}
        elif alg_name in ['lr-ss-gcn', 'ft-ss-gcn']:
            alg2type = {'lr-ss-gcn': 'linear_readout','ft-ss-gcn': 'fine_tunning'}
            alg_param['ss_params'] = {'type': alg2type[alg_name], 
                                       'theta0': theta0,
                                       'weight_decay': wd, 
                                       'maxit': maxit, 'lr': lr}
        elif alg_name in create_and_return_proposed_alg_list(['RLP','2-GCN','2-SGCN'],['2','1','inf']):
            #alg_param = {}            
            input_dim = n_input_units
            radius = input_dim*1e6
            w0 = np.zeros((input_dim,1))
            gamma0 = 1
            
            for basis_name in  ['RLP','2-GCN','2-SGCN']:
                if basis_name in alg_name:
                    for norm in ['2','1','inf']:
                        if norm in alg_name:
                            alg_param['basis_params']= (basis_name, input_dim)
                            alg_param['C_params'] = (norm,w0,radius)
                            alg_param['maxit'] = maxit #n_input_units
                            alg_param['gamma0'] = gamma0
        elif 'KMeans' in alg_name:
            input_dim = n_input_units            
            for basis_name in  ['RLP','2-GCN','2-SGCN']:
                if basis_name in alg_name:
                    alg_param['basis_params']= (basis_name, input_dim)
                    #alg_param['maxit'] = maxit #n_input_units
                    #alg_param['gamma0'] = gamma0

        return alg_param

def plot_and_save_dataset(dataset,segments,y_train,idx_train):

        y = np.empty(np.unique(segments.flatten()).shape)
        y[:] = np.nan
        y[idx_train] = y_train
        train_img = y[segments.flatten()-1].reshape(segments.shape)

        dataset_name = dataset['metadata']['name']
        satellite_name_t0 = dataset['metadata']['pre-event captured by']
        satellite_name_t1 = dataset['metadata']['post-event captured by']


        fig, ax = plt.subplots(1, 4, figsize=(10, 10), sharex=True, sharey=True)
        ax[0].imshow(np.mean(dataset['before'],axis=2) if dataset['before'].ndim >2 else dataset['before'])
        #ax[0].set_title(satellite_name_t0)
        #ax[0].set_xlabel('Jan 5, 2017')
        #ax[0].set_xlabel('pre-event')
        ax[1].imshow(np.mean(dataset['after'],axis=2) if dataset['after'].ndim >2 else dataset['after'])
        #ax[1].set_xlabel('Feb 18, 2017')
        #ax[1].set_xlabel('post-event')
        #ax[1].set_title(satellite_name_t1)
        ax[2].imshow(dataset['gt'])
        #ax[2].set_title('Ground truth')
        #ax[2].set_title('Change map')
        ax[3].imshow(np.mean(mark_boundaries(train_img, segments),axis=2))
        #ax[3].set_xlabel('Superpixel boundaries and annotated samples')
        #ax[3].set_title('Annotated superpixels')

        titles = ['Pre-event','Post-event','Change map','Annotated superpixels']
        for i in range(4):
            if 'Alaska' in dataset_name:
                ax[i].set_title(titles[i])
            ax[i].set_xticks([])
            ax[i].set_yticks([])   
        plt.savefig( "./figs\\"+ "fig_dataset_"+ dataset_name +".png", dpi=600, bbox_inches='tight')

# Experiments

def run_exp_001(X,W,f,f_ask,segments,exp_params = {'label_fraction':1/16,'ntrials':10,'alg_list':['LP']}):


    n, d = X.shape
    # Define number of trials and training set sizes
    n_trials = exp_params['ntrials']
    n_pos = np.sum(f)
    m = np.max([1,int(np.ceil(n_pos*exp_params['label_fraction']))])
    alg_list = exp_params['alg_list']

    
    kappa_table = np.zeros((len(alg_list),n_trials))
    tpr_table = np.zeros((len(alg_list),n_trials))
    tnr_table = np.zeros((len(alg_list),n_trials))
    
    MAXIT = 500   
    n_input_units = int(n/4)
    #n_input_units = n
    
    # Define learning and architecture params for GCN and SGCN models
    wd = 1e-4
    lr = 1e-2
    n_hidden_units = 16 #*4
    n_layers = 2 # K param in sgcn  

    if 'result_file_path' in exp_params.keys():
        result_file_path = exp_params['result_file_path']
        rfile = open(result_file_path, 'a')
        rfile.write('\nProposed alg params:\n')
        rfile.write('-Number of coefficients (k):'+str(np.sum(n_input_units))+'\n')
        rfile.write('-Number of iterations (N):'+str(np.sum(MAXIT))+'\n')
        #rfile.write('-Gamma0 :'+str(np.sum(MAXIT))+'\n')
        rfile.write('\nComparing alg params:\n')
        rfile.write('-Weight decay (wd):'+str(np.sum(wd))+'\n')
        rfile.write('-Learning rate:'+str(np.sum(lr))+'\n')
        rfile.write('-Hidden units (2-Layer GCN):'+str(np.sum(n_hidden_units))+'\n')
        rfile.write('-Number of layers (SGCN):'+str(np.sum(n_layers))+'\n')
        rfile.close()


    df = pd.DataFrame(index = alg_list, columns = ['kappa','recall','precision'])

    # Define seeds for training set random sampling
    seeds = np.linspace(0,1000*n_trials,n_trials).astype(int)
    stype = 'stratified_random_sampling'

    best_prob_map = {}
    kappa_best = np.zeros((len(alg_list),1)).flatten()

    for trial in range(0,n_trials):
        # Generate training and validation examples using a certain seed
        print(seeds[trial])
            
        sampling_params = {'type': stype, 
                            'seed' : seeds[trial], 
                            'labels': f}

        y_train, idx_train = gcd_utils.generate_train_dataset(m,n,sampling_params)
        y_test, idx_test = [],[] #gcd_utils.generate_validation_dataset(idx_train,int(0.2*np.sum(f)),n,sampling_params)
        #y_test = y
        #    idx_test = np.arange(0,len(y)).flatten()
        m_noise = 0 # number of corrupted labels
        y_train = gcd_utils.add_random_flip_noise(y_train,m_noise,seed=5)

        for alg_idx in range(len(alg_list)):

            print(alg_list[alg_idx])
            alg_param = parse_alg_params(alg_list[alg_idx],f,
                                            n_input_units,
                                            n_hidden_units,
                                            n_layers,wd, 
                                            MAXIT, lr, theta0=None) 

            if alg_param['name'] == 'LP':
                f_hat, p_ask = gcd_models.gsm_estimation(y_train,idx_train,W)
                probs_hat = np.nan
            elif alg_param['name'] in ['2-GCN','2-SGCN']:           
                f_hat, probs_hat, history, model = gcn_models.run(y_train, idx_train, y_test, idx_test, W, X, alg_param)
            else: # Proposed methods                  
                #f_hat, probs_hat = gcd_models.run(y_train, idx_train, y_test, idx_test, W, X, alg_param)
                basis_name, input_dim = alg_param['basis_params']
                if basis_name in ['2-GCN','2-SGCN']:
                    alg_param['feature_matrix'] =  X
                if not 'KMeans' in alg_param['name']:
                    f_hat, probs_hat = gcd_vi_models.semisup_cd_estimation(y_train,idx_train,W,alg_param)
                else:
                    nclusters = 2
                    U = gcd_utils.construct_subspace(W,param=alg_param)
                    f_hat, class_labels, class_counts = gcd_models.sc_labeler(U,nclusters,seed=1604)
                    probs_hat = np.nan
                
                f_hat, probs_hat = gcd_vi_models.semisup_cd_estimation(y_train,idx_train,W,alg_param)
            
            c_hat = gcd_utils.spixels_upsampling(f_hat, segments)

            kappa, tpr, tnr, conf_img_dict = gcd_utils.kappa_coeff(f_ask.flatten().astype(bool), c_hat.flatten().astype(bool) )

            if kappa < 0 and 'KMeans' in alg_param['name']:
                c_hat = 1 - c_hat
                kappa, tpr, tnr, conf_img_dict = gcd_utils.kappa_coeff(f_ask.flatten().astype(bool), c_hat.flatten().astype(bool) )


            # keep estimate that leads to higher kappa in the number of trials:                                 
                    
            if trial > 0 and kappa_best[alg_idx] < kappa:
                if not alg_param['name'] in ['LP','KMeans(RLP)','KMeans(2-GCN)']:
                    best_prob_map[alg_param['name']] = (seeds[trial],probs_hat)
                else:
                    best_prob_map[alg_param['name']] = (seeds[trial],f_hat)
                kappa_best[alg_idx] = kappa

            elif trial == 0:
                if not alg_param['name'] in ['LP','KMeans(RLP)','KMeans(2-GCN)']:
                    best_prob_map[alg_param['name']] = (seeds[trial],probs_hat)
                else:
                    best_prob_map[alg_param['name']] = (seeds[trial],f_hat)
                kappa_best[alg_idx] = kappa   
                best_prob_map['segments'] = segments 

            kappa_table[alg_idx,trial] = kappa
            tpr_table[alg_idx,trial] = tpr
            tnr_table[alg_idx,trial] = tnr


    n_algs = len(df['kappa'])
    tables_tuple = (kappa_table,tpr_table,tnr_table)
    for i in range(len(tables_tuple)):
        column_i = np.hstack((np.nanmean(tables_tuple[i],axis=1).reshape((n_algs,1)), 
                                np.nanstd(tables_tuple[i],axis=1).reshape((n_algs,1))))
        df[df.columns[i]] = tuple(map(tuple,np.round(column_i,2)))

    #update_dataframe(df, (kappa_table, tpr_table, tnr_table))

    return df, best_prob_map
        


