from json import tool
from unicodedata import decimal
import numpy as np
import matplotlib.pyplot as plt

import scipy as sp
import scipy.sparse.linalg as sp_s_linalg
import scipy.linalg as sp_linalg

import utils.opt_algs as opt_algs
import utils.gcd_utils as gcd_utils

import models.gcd_models as gcd_models

from scipy.sparse import coo_matrix, hstack, vstack, find
from scipy.sparse.linalg import spsolve

from sklearn.metrics import accuracy_score,balanced_accuracy_score

import torch
import torch.nn.functional as functional
#import sys  
# adding Folder_2 to the system path
#sys.path.insert(0, 'C:\\Users\\juanf\\.conda\\envs\\gcnn_env\\Lib\site-packages\\torch_sparse\\_convert_cuda.pyd')
from torch_geometric.nn import GCNConv, SGConv
from torch.nn import Linear


class GCN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, ss_pretrain = False):
        super().__init__()
        self.conv1  = GCNConv(in_channels,     hidden_channels, add_self_loops=True, bias=False)
        if not ss_pretrain:
            self.conv2  = GCNConv(hidden_channels, out_channels, add_self_loops=True, bias=False)
        else:
            self.linear = Linear(hidden_channels,out_channels,bias=False)
        self.dropout_rate = 0
        self.required_relu = True
        self.pretrain = ss_pretrain

    def set_dropout_rate(self,dropout_rate):
        self.dropout_rate = dropout_rate
    def set_required_relu(self,required_relu):
        self.required_relu = required_relu
    #def enable_pretraining(self):
    #    self.pretraining = True
    #def disable_pretraining(self):
    #    self.pretraining = False

    def extract_features(self,x,edge_index,edge_weight):
        x = self.conv1(x, edge_index, edge_weight)
        x = functional.dropout(x, p = self.dropout_rate, training=self.training)
        x = functional.relu(x) if self.required_relu else x  
        return x     

    def forward(self, x, edge_index, edge_weight):
        #x, edge_index = data.x, data.edge_index
        x = self.conv1(x, edge_index, edge_weight)
        x = functional.dropout(x, p = self.dropout_rate, training=self.training)
        x = functional.relu(x) if self.required_relu else x

        if not self.pretrain:
            x = self.conv2(x, edge_index, edge_weight)
        else:
            x = self.linear(x)

        #return x
        return functional.log_softmax(x, dim=1)
        #return x#functional.softmax(x,dim=1)

class SGCN(torch.nn.Module):
    def __init__(self, in_channels, out_channels, K):
        super().__init__()
        self.conv = SGConv(in_channels, out_channels, K, add_self_loops=True,bias=False)
        #self.conv2 = GCNConv(hidden_channels, out_channels,add_self_loops=True,bias=False)
        #self.dropout_rate = 0

    def forward(self, x, edge_index, edge_weight):
        #x, edge_index = data.x, data.edge_index
        x = self.conv(x, edge_index, edge_weight)
        return functional.log_softmax(x, dim=1)
    
class GCN_backend(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels,add_self_loops=True,bias=False)
        self.conv2 = GCNConv(hidden_channels, out_channels,add_self_loops=True,bias=False)
        self.dropout_rate = 0

    def set_dropout_rate(self,dropout_rate):
        self.dropout_rate = dropout_rate

    def forward(self, x, edge_index, edge_weight):
        x = self.conv1(x, edge_index, edge_weight)
        x = functional.dropout(x, p = self.dropout_rate, training=self.training)
        x = functional.relu(x)
        x = self.conv2(x, edge_index, edge_weight)
        return x
        #return functional.log_softmax(x, dim=1)

class SGCN_backend(torch.nn.Module):
    def __init__(self, in_channels, out_channels, K):
        super().__init__()
        self.conv1 = SGConv(in_channels, out_channels, K, add_self_loops=True,bias=False)

    def forward(self, x, edge_index, edge_weight):
        x = self.conv1(x, edge_index, edge_weight)
        return x
        #return functional.log_softmax(x, dim=1)

def initialize_model(model,type,seed):
    rng = np.random.RandomState(seed) #v001
            #theta0 = rng.randn(n_hidden_units,d)/np.sqrt(d) #n_hidden_units
    if isinstance(model,GCN) or isinstance(model,SGCN):
        for param_name, param in model.named_parameters():
            rows, cols = param.data.shape
            if type == 'Gaussian':
                param.data = torch.from_numpy(rng.randn(rows,cols)/np.sqrt(cols)).float()
            elif type == 'Uniform':
                param.data = torch.from_numpy(2*rng.rand(rows,cols)/np.sqrt(cols)-1/np.sqrt(cols)).float()
            elif type == 'Glorot':
                init_range = np.sqrt(6.0/(rows+cols))
                param.data = torch.from_numpy(2*rng.rand(rows,cols)*init_range - init_range).float()

def initialize_training(model,ss_params):

    if 'warm_start_path' in ss_params.keys():
        model.load_state_dict(torch.load(ss_params['warm_start_path']), strict=True)

    if 'weight_decay' in ss_params.keys():
        wd = ss_params['weight_decay']
    else:
        wd = 1e-3

    if 'maxit' in ss_params.keys():
        maxit = ss_params['maxit'] 
    else:
        maxit = 400

    #if 'lr' in ss_params.keys():
    lr = ss_params['lr'] if ('lr' in ss_params.keys()) else 0.01
    

    return lr, wd, maxit

def initialize_optimizer(model,lr,wd,ss_params):

    if not ss_params['ss_pretrain']:
        param_group = []
        for param_name, param in model.named_parameters():
            if 'conv1' in param_name:
                if ss_params['type'] == 'linear_readout':                
                    param_group.append({'params': param, 'weight_decay': wd})
                elif ss_params['type'] == 'fine_tunning':
                    param_group.append({'params': param, 'lr':lr/10, 'weight_decay': wd})                
            elif 'conv2' in param_name:
                param_group.append({'params': param, 'weight_decay': wd})        

    #optimizer = torch.optim.Adamax(model.parameters(), lr=0.01, weight_decay=1e-3)
    #optimizer = torch.optim.Adamax(param_group, lr=0.01, weight_decay = wd, betas=(0.7,0.999))
        optimizer = torch.optim.Adamax(param_group, lr=lr, weight_decay = wd)
    else:

        param_group = []
        for param_name, param in model.named_parameters():
            if 'linear' in param_name:
                param_group.append({'params': param, 'weight_decay': wd*ss_params['constant']})
            else:
                param_group.append({'params': param})
        optimizer = torch.optim.Adamax(param_group, lr=lr, weight_decay = wd)
        

    return optimizer

def compute_validation_loss(model, X, edge_indices,edge_weights, idx_val, y_val):
    model.eval()
    out = model(X,edge_indices,edge_weights)
    val_loss = functional.nll_loss(out[idx_val,:], y_val,reduction='mean').data
        #history['train_accuracy'][epoch] = balanced_accuracy_score(y_train,out.detach()[idx_train,:].argmax(dim=1))
        #history['test_accuracy'][epoch] = balanced_accuracy_score(y_test,out.detach()[idx_test,:].argmax(dim=1))
    model.train()
    return val_loss

def compute_accuracy(model, X, edge_indices,edge_weights, idx_val, y_val):
    model.eval()
    out = model(X,edge_indices,edge_weights)
    #val_loss = functional.nll_loss(out[idx_val,:], y_val,reduction='mean').data
        #history['train_accuracy'][epoch] = balanced_accuracy_score(y_train,out.detach()[idx_train,:].argmax(dim=1))
    acc = balanced_accuracy_score(y_val,out.detach()[idx_val,:].argmax(dim=1))
    model.train()
    return acc

def gcn_estimation(y_train, idx_train, y_test, idx_test, n_classes, W, X, hidden_chs, ss_params = {'ss_pretrain': False}):
    """
    Parameters:
    S : sparse k by n sampling matrix, allowing the selection of training examples.
    F : original one hot encoded label matrix of shape (n, num_of_classes)
    W : graph adjancecy matrix of shape (n,n)¨
    X : feature vector matrix of shape (n,d)
    """
    n = W.shape[0] # number of nodes
    #n_classes = len(np.unique(y_train))

    X = torch.tensor(X,dtype =torch.float32)
    num_of_features = X.shape[1]

    if not ('ss_pretrain' in ss_params.keys()):
        print('"ss_pretrain" field has been added to "ss_params" and set to False.')
        ss_params['ss_pretrain'] = False

    # Creates either a two layer GCN model or a one layer GCN model with a linear projection head for
    # self-supervised pretraining of the GCN layer.
    model = GCN(num_of_features,hidden_chs,n_classes, ss_pretrain = ss_params['ss_pretrain'])
    #if ss_params['ss_pretrain']:
    #initialize_model(model,type='Glorot',seed=1604)
    initialize_model(model,type='Gaussian',seed=1604)
    #initialize_model(model,type='Uniform',seed=1604)

    for param_name, param in model.named_parameters():
        #if 'conv1' in param_name:
            print(param_name,'\n')
            print(param.data)   
    #if ss_params['ss_pretrain']:
    #    print('Self-supervised pretraining mode enabled. \n')
    #    model.enable_pretraining()

    # warm-start model and load params based on ss_params dict 
    lr, wd, maxit = initialize_training(model,ss_params)

    if  not ss_params['ss_pretrain'] and ('theta0' in ss_params):
            ft_epochs = 0
            if ss_params['type'] == 'reg':
                theta0 = ss_params['theta0'] # ss_gcn_estimation(W,nclasses_ss,X,hidden_chs)
                rho = ss_params['rho']
                for param_name, param in model.named_parameters():
                    if 'conv1' in param_name:
                        theta = param
            #elif ss_params['type'] == 'linear_readout':
                #model.set_required_relu(False)
            else: 
                # Initialize feature extrator with pretrained parameters and
                # either freeze them or allow fine tunning of them during training.
                for param_name, param in model.named_parameters():
                    if 'conv1' in param_name:
                        param.data = ss_params['theta0']
                        if ss_params['type'] == 'linear_readout':
                            param.requires_grad = False
                        elif ss_params['type'] == 'fine_tunning':
                            # 
                            ft_epochs = ss_params['epochs'] if ('epochs' in ss_params)  else maxit
                            param.requires_grad = True

    optimizer = initialize_optimizer(model,lr,wd,ss_params)  

    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min')    
    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='max') 

    # Convert adjacency matrix to pytorch format 
    output = find(W)
    edge_indices =torch.tensor(np.vstack((output[0],output[1])),dtype =torch.int64)
    edge_weights = torch.tensor(output[2],dtype = torch.float32)

    # Transform train and test label to valid pytorch tensors
    y_train = torch.from_numpy(y_train).long()
    #y_test = torch.from_numpy(y_test).long()


    history = {'loss': np.zeros((maxit,1)),
               'train_accuracy': np.zeros((maxit,1)),
               'val_acc': np.zeros((maxit,1)),
               'val_loss': np.zeros((maxit,1))}
    
    model.set_dropout_rate(0.0)
    dropout_rate = 0.0   

    # Start train loop
    model.train()
    for epoch in range(0,maxit):   

        optimizer.zero_grad()
        out = model(X,edge_indices,edge_weights)

        if ss_params['type'] == 'fine_tunning' and epoch == ft_epochs:
            for param_name, param in model.named_parameters():
                if 'conv1' in param_name:
                    param.requires_grad = False

        if ss_params['type'] == 'reg':
            # ** Check this implementation
            loss_ss = functional.dropout(((theta-theta0)**2).sum()/((1-dropout_rate)*n*hidden_chs),p=dropout_rate) 
            loss = functional.nll_loss(out[idx_train,:], y_train) + rho*loss_ss
            history['loss_reg_term'][epoch] = rho*loss_ss.data
        else: 
            loss = functional.nll_loss(out[idx_train,:], y_train,reduction='mean')
            #loss = functional.mse_loss(out[idx_train,:], functional.one_hot(y_train).float())
            # Use the following line instead of the one above if the models ouput is not passed through log_softmax
            #loss = functional.cross_entropy(out[idx_train,:], y_train,reduction='sum')

        # Record loss function value on training set at a certain epoch
        history['loss'][epoch] = loss.data
        loss.backward()
        optimizer.step()     
        #history['val_loss'][epoch] = compute_validation_loss(model, 
        #                                                     X, edge_indices,edge_weights, 
        #                                                     idx_test, y_test)  
        #history['val_acc'][epoch] = compute_accuracy(model, X, edge_indices,edge_weights, idx_test, y_test)

        #print('Epoch:', epoch,', loss:', loss.data, ',val_loss:', history['val_loss'][epoch], history['val_acc'][epoch])
        print('Epoch:', epoch,', loss:', loss.data )
        
        #scheduler.step(history['val_loss'][epoch])
        #scheduler.step(history['val_acc'][epoch])

        # Enable early stopping condition after 200 epochs 
        if epoch > 200:
            loss_obs_window = np.round(history['loss'][epoch-10:epoch],decimals=4)
            # Break training loop if there is no significant improvement over the last 10 epochs
            if len(np.unique(loss_obs_window)) == 1:
                break

    model.eval()

    #for param_name, param in model.named_parameters():
    #    if 'conv1' in param_name:
    #        print('Transfer learning verification:', np.sum((param.data-ss_params['theta0']).detach().numpy().flatten())==0)

    #f_hat = model(X,edge_indices,edge_weights).argmax(dim=1).numpy()
    probs_hat = np.exp(model(X,edge_indices,edge_weights).detach().numpy())
    f_hat = np.argmax(probs_hat,axis=1)
    probs_hat = probs_hat[:,1]

    return f_hat, probs_hat, history, model

def sgcn_estimation(y_train,idx_train,n_classes, W, X, K, ss_params = {'type': 'without'}):
    """
    Parameters:
    S : sparse k by n sampling matrix, allowing the selection of training examples.
    F : original one hot encoded label matrix of shape (n, num_of_classes)
    W : graph adjancecy matrix of shape (n,n)¨
    X : feature vector matrix of shape (n,d)
    """
    n = W.shape[0] # number of nodes
    #n_classes = len(np.unique(y_train))

    X = torch.tensor(X,dtype =torch.float32)
    
    num_of_features = X.shape[1]

    model = SGCN(num_of_features,n_classes,K)
    #initialize_model(model,type='Glorot',seed=1604)
    initialize_model(model,type='Gaussian',seed=1604)
    #initialize_model(model,type='Uniform',seed=1604)

    if 'warm_start_path' in ss_params.keys():
        model.load_state_dict(torch.load(ss_params['warm_start_path']), strict=True)

    if 'weight_decay' in ss_params.keys():
        wd = ss_params['weight_decay']
    else:
        wd = 1e-3
    if 'maxit' in ss_params.keys():
        maxit = ss_params['maxit'] 
    else:
        maxit = 400

    optimizer = torch.optim.Adamax(model.parameters(), lr=0.01, weight_decay=wd)
    #optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=wd)
    #optimizer = torch.optim.LBFGS(model.parameters(), lr=0.01, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None)

    # adjacency matrix conversion to pytorch format 
    output = find(W)
    edge_indices =torch.tensor(np.vstack((output[0],output[1])),dtype =torch.int64)
    edge_weights = torch.tensor(output[2],dtype = torch.float32)

    y_train = torch.from_numpy(y_train)

    #maxit = maxit
    #tol = 1e-2
    history = {'loss': np.zeros((maxit,1))}
    model.train()
    #model.set_dropout_rate(0.05)
    dropout_rate = 0.0
    for epoch in range(0,maxit):
        
        optimizer.zero_grad()
        out = model(X,edge_indices,edge_weights)
        loss = functional.nll_loss(out[idx_train,:], y_train,reduction='sum')
            #loss_ss = torch.Tensor(0)

        if epoch > 200:
            loss_obs_window = np.round(history['loss'][epoch-10:epoch],decimals=4)
            # check if there is no significant improvement over the last 10 epochs
            if len(np.unique(loss_obs_window)) == 1:
                break

        #if loss.data < 5e-2:
        #    break
        history['loss'][epoch] = loss.data       

        print('Epoch:', epoch,', loss:',loss.data)
        loss.backward()
        #optimizer.step()
        optimizer.step()
     
    model.eval()

    probs_hat = np.exp(model(X,edge_indices,edge_weights).detach().numpy())
    f_hat = np.argmax(probs_hat,axis=1)
    probs_hat = probs_hat[:,1]

    return f_hat, probs_hat, history, model

def run(y_train,idx_train, y_test, idx_test,W, X, algs):

    if algs['name'] == '2-GCN':
        n_classes = algs['n_classes'] 
        n_hidden_units = algs['n_hidden_units'] 
        ss_params = algs['ss_params']
        if not (ss_params['type'] == 'without'):
            raise ValueError("ss_params['type'] must be set as 'without', i.e., ss_params['type'] = 'without'.")
        f_hat, probs_hat, history, model = gcn_estimation(y_train, idx_train, y_test, idx_test,n_classes, W, X, n_hidden_units, ss_params = ss_params)

    if algs['name'] == '2-SGCN':
        n_classes = algs['n_classes'] 
        n_layers = algs['n_layers'] 
        ss_params = algs['ss_params']
        if not (ss_params['type'] == 'without'):
            raise ValueError("ss_params['type'] must be set as 'without', i.e., ss_params['type'] = 'without'.")

        f_hat, probs_hat, history, model = sgcn_estimation(y_train,idx_train, n_classes, W, X, n_layers, ss_params = ss_params)
    
    if algs['name'] in ['lr-ss-gcn','ft-ss-gcn']:
        n_classes = algs['n_classes'] 
        n_hidden_units = algs['n_hidden_units'] 
        ss_params = algs['ss_params']

        if not (((ss_params['type'] == 'linear_readout') or (ss_params['type'] == 'fine_tunning'))
                                           and ('theta0' in ss_params.keys())):
            raise ValueError("Either ss_params['type'] different to 'tf' or ss_params['theta0'] undefined.")
        
        f_hat, probs_hat, history, model = gcn_estimation(y_train, idx_train, y_test, idx_test, n_classes, W, X, n_hidden_units, ss_params = ss_params)
   
    return f_hat, probs_hat, history, model