from genericpath import exists

import numpy as np
import pandas as pd

import os
import time
import pickle
import matplotlib.pyplot as plt

import utils.gcd_utils as gcd_utils 
import models.gcd_models as gcd_models
import models.gcd_vi_models as gcd_vi_models



 
#dirname = os.path.dirname(__file__)
#datasets_dir = os.path.join(dirname, 'datasets')

def parse_alg_params(n,epsilon,dict_name,norm,scheme):
    alg_param = {}
    if scheme == 'unsupervised':
        #alg_param['C_params'] = (epsilon,norm)
        alg_param['C_params'] = (norm,np.zeros((n,1)),epsilon) 
        if dict_name in ['2-GCN','2-SGCN']:
            alg_param['basis_params'] = (dict_name, n)# int(n/16))  
            #alg_param['feature_matrix'] = X_mean[0] 
        elif dict_name == 'RLP':
            alg_param['basis_params'] = (dict_name, n)  
        else:
            raise ValueError('alg_param must be set using a valid configuration.')
    elif scheme == 'semisupervised':
        alg_param['C_params'] = (norm,np.zeros((n,1)),epsilon)    
        alg_param['basis_params'] = (dict_name, n) 

    return alg_param

####


def run_and_log_expt_results(dataset_names,datasets_dir,results_dir):

    """ Implement experiment in Sec. VI-D Ablation study on the number of superpixels and structure
    learning algorithm, and store results for posterior visualization. """

    
    if os.path.exists(results_dir):
        print('Experiment was not carried out due to possible existing records and information loss.')
        return
    else:
        os.mkdir(results_dir)

    out_fn = lambda db_idx,db_name, ext: 'Exp_003_ablation_study_on_db_'+ str(db_idx) + '_' + db_name + ext

    gl_models = ['Kalofolias', 'MST_with_binary_weights']
    #gl_models = ['MST_with_binary_weights']
    #gl_models = ['Kalofolias']
    num_superpixels = np.logspace(2,4,5).astype(int)

    filename_format = '{i}_{scheme}_results_on_{db}_with_{glmodel}_{dict_name}_{norm}'
    for dataset_idx in [0]:#range(1,len(dataset_names)):

        # Load change detection dataset
        dataset = gcd_utils.dataset_loader(datasets_dir,dataset_names[dataset_idx])     

        # Create dataset from image pairs based on slic superpixels
        for i in range(len(num_superpixels)):
            
            # Apply preprocessing pipeline
            n_spixels = num_superpixels[i]
            context_radius = 3
            if dataset['metadata']['name'] in ['California','Mulgaria','Alaska','Atlantico','Toulouse-1']:
                prepro_params = {'sp_method':'slic','iqr_clipping' : True}
            else: #'Shuguang'
                prepro_params = {'sp_method':'slic','iqr_clipping' : False}
            X_mean, X, F, segments = gcd_utils.prepro_pipeline(dataset,n_spixels,context_radius,prepro_params)

            # Construct graph on which labels are assumed to be smooth    
            n = X[0].shape[0]
            d = X[0].shape[1]+X[1].shape[1]
            k = int(np.sqrt(n))

            for gl_model_idx in range(0,len(gl_models)):
                #scheme = 'unsupervised' 
                log_fn = out_fn(dataset_idx,dataset['metadata']['name'],'.txt')
                # Define gl_params for main graph inferred from pre and post event avg. signals. The other graph
                # is assumed to be a knn graph with Gaussian weights.
                gl_params = {'model': gl_models[gl_model_idx],
                            'k': k,
                            'knn_edge_cons': True,
                            'k_': 5*k,
                            'fusion_rule': 'sum',
                            'tol': 1e-5
                            }
                W, W1, outparams, outparams1 = gcd_utils.structure_learning_pipeline(X_mean,'unsupervised',gl_params,
                                                                                        log_fn,results_dir)
      
                epsilon = n*1e5 # just a very large number initially
                dict_name = 'RLP'
                norm = '2'

                for scheme in ['semisupervised','unsupervised']:
                    alg_param = parse_alg_params(n,epsilon,dict_name,norm,scheme)
                    alg_param['maxit'] = int(n/2)
                    alg_param['reversed'] = False
                    alg_param['gamma0'] = 1

                    # Convert graph signals to image
                    if scheme == 'semisupervised':
                        sampling_params = {'type': 'stratified_random_sampling', 
                                        'seed' : 1094, 
                                        'labels': F[:,0]}
                        m = np.max([int(0.065*np.sum(sampling_params['labels'])),1])
                        y_train, idx_train = gcd_utils.generate_train_dataset(m,n,sampling_params)
                        f_hat, probs_hat = gcd_vi_models.semisup_cd_estimation(y_train,idx_train,W,alg_param)
                        score_map = gcd_utils.spixels_upsampling(probs_hat, segments) 
                        
                        output = {'probs_hat':probs_hat}
                    else:                    
                        x1_hat, delta, history = gcd_vi_models.unsupervised_cd_estimation(X_mean[1],W1,W,alg_param)
                        delta_mag = np.sqrt(np.sum((delta)**2,axis=1)).reshape((n,1))    
                        score_map = gcd_utils.spixels_upsampling(delta_mag, segments) 
                        #score_map = delta_mag
                        output = {'x1_hat':x1_hat,
                                'delta':delta,
                                'history':history}
        
                    ref_fpr = 0.05
                    metrics_dict = gcd_utils.compute_performance_metrics(dataset['gt'], score_map, ref_fpr)
                    #metrics_dict = gcd_utils.compute_performance_metrics(F[:,0], score_map, ref_fpr)
                    
                    #trial = '{sche}_results_on_{db}_with_{glmodel}_{dict_name}_{norm}'
                    result_fn = filename_format.format(i=i,scheme=scheme,
                                        db=dataset['metadata']['name'],
                                        glmodel=gl_models[gl_model_idx],
                                        dict_name=dict_name,
                                        norm=norm)+'.pkl'
                    output['metrics_dict'] = metrics_dict
                    output['segments'] = segments
                    output['num_spixels_ref'] = n_spixels
                    output['num_slic_spixels'] = n

                    with open(os.path.join(results_dir, result_fn), 'wb') as r2f:
                        pickle.dump(output, r2f)


## Display functions

def unpack_ablation_study_results(results_dir,filename_format,list_of_configs,dict_name,norm):

    for config in list_of_configs:
        file_name = filename_format.format(i=config[2],scheme=config[3],db=config[0],glmodel=config[1],
                        dict_name=dict_name,norm=norm)
        if os.path.isfile(os.path.join(results_dir,file_name+'.pkl')):
            with open(os.path.join(results_dir,file_name+'.pkl'), 'rb') as f:
                file_data = pickle.load(f)
            
            new_row = {'n_spixels_ref': file_data['num_spixels_ref'],
                    'n_spixels': file_data['num_slic_spixels'],
                    'dataset'  : config[0],
                    'gl_model' : config[1],
                    'scheme'   : config[3]}
            for idx in range(len(file_data['metrics_dict']['metrics_labels'])):
                new_row[file_data['metrics_dict']['metrics_labels'][idx]] = np.round(file_data['metrics_dict']['metrics'][idx],2)
            if 'table' in locals():
                table = pd.concat([table,pd.DataFrame(new_row,index=[0])],axis=0,ignore_index=True)
            else: 
                table = pd.DataFrame(new_row,index=[0])      
    return table        

def gen_color_dict(cmap_name, categorical_keys):
    """
    Parameters:
    categorical_keys (list): denotes a list of string values that will act as keys in the color
    asignment dictionary.
    """
    from matplotlib.cm import get_cmap
    cmap = get_cmap(cmap_name)
    colors = cmap(np.linspace(0, 1, len(categorical_keys)))
    color_dict = { categorical_keys[i] : colors[i,:] for i in range(len(categorical_keys))}

    return color_dict

def create_barplot(db_name,df,num_features, in_cat_feature, out_cat_feature,save_dir):    
    # create len(out_barplot_cat) independent barplots

    # Set the colormap
    cmap_name = 'tab20'
    from matplotlib.cm import get_cmap
    cmap = get_cmap(cmap_name)
    
    out_cat_values = df.loc[:,out_cat_feature].unique()
    in_cat_values = df.loc[:,in_cat_feature].unique()
    color_dict = gen_color_dict(cmap_name, in_cat_values)

    x_axis = np.arange(len(num_features))
    width = 0.4
    
    for out_c_val in out_cat_values: # outer categories loop
        # reduce rows to those with outer category c
        tmp = df.loc[df.loc[:,out_cat_feature]==out_c_val,:]
        #in_cat_values = df.loc[:,in_cat_feature].unique()
        #colors = cmap(np.linspace(0, 1, len(in_cat_values)))
        #color_dict = { in_cat_values[i] : colors[i,:] for i in range(len(in_cat_values))}
        
        num_bars = len(in_cat_values)
        # compute bar centers
        if num_bars == 2:
            barwidth = 0.4
            bar_shift = { in_cat_values[0] : -0.2,  in_cat_values[1]: 0.2}
        else: # TODO: generalize to more bars
            barwidth = 0.8 # default value
            bar_shift = { in_cat_values[i] : 0 for i in range(len(in_cat_values))}
        fig, ax = plt.subplots()
        for in_c_val in in_cat_values:
            filter = tmp.loc[:,in_cat_feature] == in_c_val
            avg_num_features = tmp.loc[filter,num_features].mean(axis=0).to_numpy()
            std_num_features = tmp.loc[filter,num_features].std(axis=0).to_numpy()        
            ax.bar(x = x_axis + bar_shift[in_c_val], width = barwidth, height=avg_num_features, yerr=std_num_features, label = in_c_val,color=color_dict[in_c_val])
            
            if not 'yticks' in locals():
                yticks = avg_num_features
            else:
                yticks = np.maximum(avg_num_features, yticks)

        plt.xticks(x_axis, num_features, fontsize=20, rotation=45)
        #ax.set_yticks(np.round(yticks,2))
        ax.set_yticks(np.linspace(0,1,5))
        ax.set_ylim(0,1)
        ax.set_box_aspect(1)
        #ax.set_yscale('log')
        ax.legend(loc="best")
        ax.set_ylabel('Avg. value', fontsize = 20)        
        ax.set_title(out_c_val,fontdict={'fontsize':20})

        DPI = 400
        plt.savefig(save_dir + "\\fig_ablation_bar_imgs_"+ "_{db}_{schm}_".format(db=db_name,schm=out_c_val) + ".png", dpi=DPI, bbox_inches='tight')

        #return fig, ax

def plot_num_feature(db_name, df,x_num_feature,y_num_feature, in_categ_feature, out_categ_feature,save_dir):
    """ creates a subplot of size (1,num_categ_values), where num_categ_values is given by
    the unique values of the column 'out_categ_feature' in df. Each subplot consists of curves
    associated with the different categories in 'in_categ_feature'.
    
    Paramters:
    df (dataframe): denotes a table with numerical and categorical features
    x_num_feature,y_num_feature, in_categ_feature, out_categ_feature (string): denote strings
    in the columns of df.
    """

    in_categ_values = df[in_categ_feature].unique()
    cmap_name = 'rainbow'
    color_dict = gen_color_dict(cmap_name, list(in_categ_values))
    #marker = {'Kalofolias':'ro--','MST_with_binary_weights':'bs-'}
    #import itertools
    available_markers = ['o--', '+-', 's', ',', '*']
    markers = {in_categ_values[i]:available_markers[i % len(available_markers)] for i in range(len(in_categ_values))}

    out_categ_values = df[out_categ_feature].unique()
    fig,ax = plt.subplots(1,len(out_categ_values), figsize=(10, 10), sharey=True, sharex=True)
    for idx_sch in range(len(out_categ_values)):
        table_scheme = df.loc[df.loc[:,out_categ_feature]== out_categ_values[idx_sch],:]
        for gl_name in df[in_categ_feature].unique():
            x = table_scheme.loc[table_scheme.loc[:,in_categ_feature]==gl_name, x_num_feature]
            y = table_scheme.loc[table_scheme.loc[:,in_categ_feature]==gl_name, y_num_feature]
            #ax[idx_metric,idx_sch].semilogx(x,y,marker[gl_name],label=gl_name + '({scheme})'.format(scheme=schemes[idx_sch]),
            #                        color=color[schemes[idx_sch]])
            ax[idx_sch].set_box_aspect(1)
            ax[idx_sch].semilogx(x,y,markers[gl_name],color=color_dict[gl_name],label=gl_name)
            ax[idx_sch].set_title(out_categ_values[idx_sch],fontdict = {'fontsize':20})
        ax[idx_sch].legend(loc='best', borderaxespad=0., fontsize = 'small')#,ncol=4)


        ax[idx_sch].set_xlabel(x_num_feature, fontsize = 20)
        ax[idx_sch].set_ylabel(y_num_feature, fontsize = 20)
        ax[idx_sch].tick_params('y',labelleft=True)

    DPI = 400
    plt.savefig(save_dir + "\\fig_ablation_plots_imgs_"+ db_name + ".png", dpi=DPI, bbox_inches='tight')

    #return fig, ax

def display_expt_results(dataset_names,results_dir):


    gl_models = ['Kalofolias', 'MST_with_binary_weights']
    num_superpixels = np.logspace(2,4,5).astype(int)
    schemes = ['unsupervised','semisupervised']
    list_of_configs = [(db,gl,i,scheme) for db in dataset_names for gl in gl_models for i in range(0,len(num_superpixels)) for scheme in schemes]
    dict_name = 'RLP'
    norm = '2'
    filename_format = '{i}_{scheme}_results_on_{db}_with_{glmodel}_{dict_name}_{norm}'
    table = unpack_ablation_study_results(results_dir,filename_format,list_of_configs,dict_name,norm)

    # Generate plots and store results in figs
    DPI = 400
    img_dir = f"./figs/figs_ablation_study_res_{DPI}"
    timestamp = time.strftime("%Y%m%d%H%M%S")
    full_img_dir = f"{img_dir}_{timestamp}"

    if not os.path.exists(full_img_dir):
        os.mkdir(full_img_dir)
    else:
        # Handle the case when the directory already exists
        print(f"Directory '{full_img_dir}' already exists.")

    x_num_feature = 'Number of superpixels'
    y_num_feature = 'AUC'
    in_categ_feature = 'gl_model'
    out_categ_feature = 'scheme' 
    params = {'fontsize':15,'xlabel':'Number of superpixels','ylabel':'AUC'}
    # Rename column in table so it can be accessed by x_num_feature
    table = table.rename(columns={'n_spixels':'Number of superpixels'})
    save_dir = img_dir

    for db_name in table['dataset'].unique():

        db_filter = table['dataset']==db_name
        table_for_db = table.loc[db_filter,:]
        plot_num_feature(db_name,table_for_db,x_num_feature,y_num_feature, in_categ_feature, out_categ_feature,save_dir)

        metrics_labels = ['AUC','F1-SCORE','KAPPA','TPR_REF','FPR_REF']
        diff_barplot_cat = schemes # 'semisup', 'unsup'
        out_cat_feature = 'scheme'
        in_cat_feature = 'gl_model'
        num_features = ['AUC','F-SCORE','KAPPA','TPR_REF','FPR_REF']
        create_barplot(db_name,table_for_db,metrics_labels,in_cat_feature,out_cat_feature,save_dir)

        print('fin')




            
