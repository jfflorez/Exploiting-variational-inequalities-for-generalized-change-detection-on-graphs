from genericpath import exists

import pandas as pd
import numpy as np
import pickle
import os 
import matplotlib.pyplot as plt

from skimage.filters import threshold_multiotsu

import utils.gcd_utils as gcd_utils 
import models.gcd_vi_models as gcd_vi_models





def run_and_log_expt_results(dataset_names,datasets_dir,results_dir):
    """ Implement experiment in Sec. VI-C Unsupervised change map estimation using signal decom-
    position, and store results for posterior visualization. """

    if os.path.exists(results_dir):
        print('Experiment was not carried out due to possible existing records and information loss.')
        return
    else:
        os.mkdir(results_dir)
    #dirname = os.path.dirname(__file__)
    #datasets_dir = os.path.join(dirname, 'datasets')
    out_fn = lambda db_idx,db_name, ext: 'Exp_002_unsupervised_on_db_'+ str(db_idx) + '_' + db_name + ext
    #results_dir = os.path.join(dirname, 'results')
    #results_dir = os.path.join(results_dir,'FinalResults_13_02_2023')

    #dataset_names = ['Alaska','California','Atlantico', 'Mulgaria','Toulouse-1','Shuguang']
    for dataset_idx in range(0,len(dataset_names)):

        dataset_name = dataset_names[dataset_idx]
        if os.path.isfile(os.path.join(results_dir,out_fn(dataset_idx,dataset_name,'.txt'))):
            continue

        # Load dataset
        dataset = gcd_utils.dataset_loader(datasets_dir,dataset_names[dataset_idx])
    
        # Create dataset from image pairs based on slic superpixels
        #n_spixels = 1500
        n_spixels = 2400
        #n_spixels = 2400
        #n_spixels = 1000
        context_radius = 3
        sp_method = 'slic'
        outlier_removal = True
        if dataset['metadata']['name'] in ['California','Mulgaria','Alaska','Atlantico','Toulouse-1']:
            prepro_params = {'sp_method':'slic','iqr_clipping':True}
        else: #'Shuguang'
            prepro_params = {'sp_method':'slic','iqr_clipping':False}
        #sp_method = 'watershed'
        X_mean, X, F, segments = gcd_utils.prepro_pipeline(dataset,n_spixels,context_radius,prepro_params)

        #x1 = X_mean[0]
        #x2 = X_mean[1]
        #X_mean = (x1 - x1.mean(),x2 - x2.mean())
        #del x1,x2   

        reversed_flag = False if not dataset['metadata']['name'] == 'Atlantico' else True
        if reversed_flag:
            x1 = X_mean[0]
            x2 = X_mean[1]
            X_mean = (x2,x1)
        #del x1,x2

        # Define ground truth labeling function
        f_ask = dataset['gt']
        # Define training labels 
        f = 1 - np.argmax(F,axis=1)
        n_pos = np.sum(f)
        n_neg = np.sum(1-f)

        ## Construct graph on which labels are assumed to be smooth
        
        #if isinstance(X,tuple):
            #X = np.hstack(X)
            #scale = np.max(X,axis=0).reshape((1,X.shape[1]))
            #X = X/scale
            #X_mean = np.hstack(X_mean)

        n, d2 = X[1].shape    
        k_list = [int(np.sqrt(n))]
        k_idx = 0

        mode = 'unsupervised'
        log_fn = out_fn(dataset_idx,dataset['metadata']['name'],'.txt')
        # Define gl_params for main graph inferred from pre and post event avg. signals. The other graph
        # is assumed to be a knn graph with Gaussian weights.
        gl_params = {'model': 'Kalofolias', # or 'Gaussian'
                    'k': int(k_list[k_idx]),
                    'knn_edge_cons': True,
                    'k_': 5*k_list[k_idx],
                    'fusion_rule': 'sum',
                    'tol': 1e-5
                    }
        W, W1, outparams, outparams1 = gcd_utils.structure_learning_pipeline(X_mean,mode,gl_params,
                                                                            log_fn,results_dir)

        #rfile = open(results_dir + , 'w')
        #rfile.write(dataset['metadata']['name'] +'\n')  
        #rfile.write('\nGraph learning params: \n')   
        #rfile.write('-Number of nodes: ' + str(n) + '\n' )

        #gl_params = {'model': 'Gaussian', # or 'Gaussian'
        #            'k': int(k_list[k_idx]),
        #             'knn_edge_cons': True,
        #             'k_': k_list[k_idx],
        #             'fusion_rule': 'sum',
        #             }
        #rfile.write('\nGraph G1: \n')   
        #rfile.write('-Method: ' + gl_params['model'] + '\n' ) 
        #rfile.write('-Number of nearest neighbors: ' + str(gl_params['k']) + '\n' )
        #rfile.write('-Fusion rule: ' + str(gl_params['fusion_rule']) + '\n' )

        #W1, theta = gcd_utils.construct_adj_matrix(X_mean[0], gl_params)    
        #gl_params['fusion_rule'] = 'max'

        #gl_params = {'model': 'Kalofolias', # or 'Gaussian'
        #            'k': int(k_list[k_idx]),
        #             'knn_edge_cons': True,
        #             'k_': 5*k_list[k_idx],
        #             'fusion_rule': 'sum',
        #            'tol': 1e-5
        #             }
        #rfile.write('\nGraph G: \n')   
        #rfile.write('-Method: ' + gl_params['model'] + '\n' ) 
        #rfile.write('-Number of nearest neighbors: ' + str(gl_params['k']) + '\n' )
        #rfile.write('-Tol: ' + str(gl_params['tol']) + '\n' )
        #rfile.write('-Fusion rule: ' + str(gl_params['fusion_rule']) + '\n' )
        #rfile.close()

        #W, theta = gcd_utils.construct_adj_matrix(X_mean, gl_params)

        radius = 1000
        s = 1000*n # makes sure the projection onto the l-1 ball is intactive
        epsilon = 0.1

    

        #dict_atoms_names = ['RLP','2-GCN']
        dict_atoms_names = ['2-GCN']
        norms = ['2','inf']
        alg_name = lambda dname,norm: 'Prop. Unsup. ('+dname+','+norm+')'
        alg_list = [alg_name(dname,norm) for dname in dict_atoms_names for norm in norms]
        #alg_list = ['Prop. Unsup. (RLP,2)','Prop. Unsup. (RLP,inf)']
        version = ['(single Thresh.)','(multi Thresh.)']
        aug_alg_list = [alg+v for alg in alg_list for v in version]

        df = pd.DataFrame(index = aug_alg_list, columns = ['kappa','recall','precision'])
        output = {}
        alg_param={'maxit':int(n/2), 'X1': X_mean[0], 'reversed': reversed_flag}
        
        #alg_param['basis_params'] = ('sc',int(n/2))


        for alg_i in dict_atoms_names:
            for norm_j in norms:
                alg_param['C_params'] = (radius,norm_j)
                if alg_i in ['RLP','2-GCN','2-SGCN']:                              
                    if alg_i in ['2-GCN','2-SGCN']:
                        alg_param['basis_params'] = (alg_i, n)# int(n/16))  
                        alg_param['feature_matrix'] = X_mean[0]
                    else:
                        alg_param['basis_params'] = (alg_i, n)  
                else:
                    raise ValueError('alg_param must be set using a valid configuration.')
                alg_ofile_name = alg_i+'_'+norm_j
                alg = alg_name(alg_i,norm_j) 

                    
                x1_hat, delta, history = gcd_vi_models.unsupervised_cd_estimation(X_mean[1],W1,W,alg_param)
                delta_mag = np.sqrt(np.sum((delta)**2,axis=1)).reshape((n,1))    
                delta_hat = gcd_utils.spixels_upsampling(delta_mag, segments)

                thresholds_v01 = threshold_multiotsu(delta_hat,classes=2)
                thresholds_v02 = threshold_multiotsu(delta_hat,classes=3)
                c_hat = np.digitize(delta_hat, bins=thresholds_v01)
                kappa, tpr, tnr, conf_img_dict = gcd_utils.kappa_coeff(f_ask.flatten().astype(bool), c_hat.flatten().astype(bool) )
                df['kappa'][alg+'(single Thresh.)'] = kappa
                df['recall'][alg+'(single Thresh.)'] = tpr
                df['precision'][alg+'(single Thresh.)'] = tnr    
                c_hat = np.digitize(delta_hat, bins=np.array([thresholds_v02[1]]))
                kappa, tpr, tnr, conf_img_dict = gcd_utils.kappa_coeff(f_ask.flatten().astype(bool), c_hat.flatten().astype(bool) )
                df['kappa'][alg+'(multi Thresh.)'] = kappa
                df['recall'][alg+'(multi Thresh.)'] = tpr
                df['precision'][alg+'(multi Thresh.)'] = tnr

                output['x2'] = X_mean[1]
                output['x1_hat'] = x1_hat
                output['delta'] = delta
                output['delta_mag'] = delta_mag
                output['segments'] = segments
                output['thresholds_v01'] = thresholds_v01
                output['thresholds_v02'] = thresholds_v02
                output['history'] = history

                config_ext = '_{dict_name}_{norm}'.format(dict_name=alg_i,norm=norm_j)
                with open(os.path.join(results_dir, out_fn(dataset_idx,dataset['metadata']['name'] + config_ext,'.pkl')), 'wb') as r2f:
                        pickle.dump(output, r2f)

            rfile = open(os.path.join(results_dir, log_fn), 'a') 
                #rfile.write('\nExperiment results:\n-Number of trials: '+str(n_trials)+'\n'+'-Training samples per class (m): ')
                #for i  in range(len(m)-1):
                #    rfile.write(str(m[i])+',')
                #rfile.write(str(m[len(m)-1])+'\n\n')
            rfile.write(df.to_latex())
            rfile.close()


    #exp001_utils.display_unsup_method_results(datasets_dir,results_dir,dataset_names)

def display_expt_results(dataset_names,datasets_dir,results_dir):
    """ Display previously computed results from experiment in Sec. VI-C Unsupervised change map estimation using signal decom-
    position. """

    #dirname = os.path.dirname(__file__)
    #datasets_dir = os.path.join(dirname, 'datasets')
    out_fn = lambda db_idx,db_name, ext: 'Exp_002_unsupervised_on_db_'+ str(db_idx) + '_' + db_name + ext
    #results_dir = os.path.join(dirname, 'results')
    #results_dir = os.path.join(results_dir,'FinalResults_13_02_2023')


    #dataset_names = ['Alaska','California','Atlantico', 'Mulargia','Toulouse','Shuguang']

    list_algs_filter = ['LP', '2-GCN', '2-SGCN', 'Proposed (RLP,2)','Proposed (RLP,inf)','Proposed (2-GCN,2)','Proposed (2-GCN,inf)']

    #for m_idx in range(len(m)):

    # create image directory
    DPI = 400
    img_dir = "./figs\\figs_unsupv_res_"+str(DPI)
    if not os.path.exists(img_dir):
        os.mkdir(img_dir)
                        
    for db_idx in range(len(dataset_names)):
            dataset_name = dataset_names[db_idx]
            dataset = gcd_utils.dataset_loader(datasets_dir,dataset_name) 

            file_name = out_fn(db_idx,dataset_names[db_idx],'_output_unsup_analysis'+'.pkl')

            #list_algs = list(output.keys())
            list_img = ['Estimated avg. \n pre-event img', 'Original avg. \n post-event img',  
                        "Estimated diff. \n signal's mag. img", 'Segmented \n mag. img', 'Confusion \n change map', 'Ground truth']
            list_img_reversed = ['Original avg. \n post-event img', 'Estimated avg. \n pre-event img',  
                        "Estimated diff. \n signal's mag. img", 'Segmented \n mag. img','Confusion \n  change map', 'Ground truth']
            #list_algs.remove('segments')
            #list_algs_filter = ['Prop. Unsup. (RLP,2)', 'Prop. Unsup. (RLP,inf)']
            list_algs_filter = ['Prop. Unsup. (RLP,2)', 'Prop. Unsup. (RLP,inf)','Prop. Unsup. (2-GCN,2)', 'Prop. Unsup. (2-GCN,inf)']

            for alg in list_algs_filter:
                if '(RLP,2)' in alg:
                    ext = '_RLP_2'
                    file_name = out_fn(db_idx,dataset_names[db_idx],ext+'.pkl')
                elif '(RLP,inf)' in alg:
                    ext = '_RLP_inf'
                    file_name = out_fn(db_idx,dataset_names[db_idx],'_RLP_inf'+'.pkl')
                elif '(2-GCN,2)' in alg:
                    ext = '_2-GCN_2'
                    file_name = out_fn(db_idx,dataset_names[db_idx],ext+'.pkl')
                elif '(2-GCN,inf)' in alg:
                    ext = '_2-GCN_inf'
                    file_name = out_fn(db_idx,dataset_names[db_idx],ext+'.pkl')
                output = pickle.load( open(os.path.join(results_dir,file_name),"rb"))
                segments = output['segments']

                X2 = np.reshape(np.mean(output['x2'],axis=1)[segments.flatten()-1],segments.shape)
                X1_hat = np.reshape(np.mean(output['x1_hat'],axis=1)[segments.flatten()-1],segments.shape)
                DiffSigMag = np.reshape(np.mean(output['delta_mag'],axis=1)[segments.flatten()-1],segments.shape)
                #C_hat = DiffSigMag > output['thresholds_v02'][1]
                segm_hat = np.digitize(DiffSigMag, bins=np.array(output['thresholds_v02']))
                c_hat = np.digitize(DiffSigMag, bins=[output['thresholds_v02'][1]])
                kappa, tpr, tnr, conf_img_dict = gcd_utils.kappa_coeff(dataset['gt'].flatten().astype(bool), c_hat.flatten().astype(bool) )
                n1, n2 = segments.shape
                
                #fig2, ax2 = plt.subplots(1, len(list_algs_filter)+1, figsize=(10, 10), sharex=True, sharey=True)
                fig1, ax1 = plt.subplots(1, len(list_img), figsize=(30, 10), sharex=False, sharey=True)
                #fig1, ax1 = plt.subplots(1, len(list_img), sharex=False, sharey=True)

                fontsize = 20
                ax1[0].imshow(X1_hat)
                #ax1[0].set_ylabel(dataset_name,fontdict={'fontsize':15})
                ax1[0].set_ylabel(dataset_name,fontdict={'fontsize':fontsize})
                ax1[1].imshow(X2)
                ax1[2].imshow(DiffSigMag,cmap='gist_gray')
                #fig1.colorbar(img, ax=ax1[2], location='right')#,shrink=0.35) 
                ax1[3].imshow(segm_hat)
                gcd_utils.plot_confusion_img_on_ax(conf_img_dict,n1,n2,ax1[4],add_legend=True,default_loc=True)

                #fig_tmp = plt.figure()
                #ax_tmp = fig_tmp.add_axes(ax1[4].get_position())
                
                #ax1[4].__dict__.update(ax_tmp.__dict__)
                #ax1[4]
                #ax1[4].legend(fontsize=10)
                ax1[5].imshow(dataset['gt'])
                if db_idx == 0:
                    for i in range(len(list_img)):
                        #ax1[i].set_title(list_img[i],fontdict={'fontsize':15})    
                        ax1[i].set_title(list_img[i],fontdict={'fontsize':fontsize})                 
                        ax1[i].set_xticks([])
                        ax1[i].set_yticks([])     
                elif 'Atlantico' in dataset_name: 
                    for i in range(len(list_img)):
                        #ax1[i].set_title(list_img_reversed[i],fontdict={'fontsize':15})
                        ax1[i].set_title(list_img_reversed[i],fontdict={'fontsize':fontsize})                     
                        ax1[i].set_xticks([])
                        ax1[i].set_yticks([])     
                else:
                    for i in range(len(list_img)):
                        #ax1[i].set_title(list_img[i],fontdict={'fontsize':15})                    
                        ax1[i].set_xticks([])
                        ax1[i].set_yticks([])               
                

                plt.savefig(img_dir+ "\\fig_unsupervised_result_imgs_"+ dataset_name + '_'+ ext +".png", dpi=DPI, bbox_inches='tight')
